{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e29f3a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2021-06-10T20:45:27.638165+08:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.5\n",
      "IPython version      : 7.22.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 142 Stepping 11, GenuineIntel\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "seaborn   : 0.11.1\n",
      "numpy     : 1.20.1\n",
      "watermark : 2.2.0\n",
      "pandas    : 1.2.4\n",
      "matplotlib: 3.3.4\n",
      "re        : 2.2.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas\n",
    "# !pip install watermark\n",
    "# !pip install seaborn\n",
    "# !pip install biopython\n",
    "# !pip install sklearn\n",
    "import os\n",
    "import re \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import watermark\n",
    "import random \n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all'\n",
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b2638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(dirs):\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    else:\n",
    "        pass\n",
    "mkdir(\"SVMoutput\")\n",
    "# mkdir(\"SVMpredict\")\n",
    "left=20\n",
    "right=20\n",
    "signal_num = left+right\n",
    "bases=\"ACGT\"\n",
    "lower_bases=\"acgt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f9fe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOADING Training Set: 100%|██████████████████████████████████████████████████████| 462/462 [00:00<00:00, 231956.96it/s]\n",
      "LOADING Testing Set: 100%|███████████████████████████████████████████████████████| 570/570 [00:00<00:00, 190543.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def loadFile(file_dir):\n",
    "    '''\n",
    "    Function: Read  All files in the Training Set Folder and Testing Set Folder\n",
    "    Parameter：file_dir\n",
    "    Output: file_path,locus_list\n",
    "    Attention: do not load non-fasta files!\n",
    "    '''\n",
    "    file_path = []\n",
    "    file_locus_list = []\n",
    "    all_file = tqdm(os.listdir(file_dir), desc=f'LOADING {file_dir}')\n",
    "    for file_name in all_file:\n",
    "        suffix = re.findall(\"\\.(.+$)\",file_name)[-1].lower()\n",
    "        # or  suffix = file_name.split(\".\")[1].lower()\n",
    "        if suffix != \"txt\":\n",
    "            continue\n",
    "        path = f\"{file_dir}/{file_name}\"\n",
    "        file_path.append(path)\n",
    "    all_file.close()\n",
    "    return file_path\n",
    "\n",
    "train_file_path =loadFile('Training Set')\n",
    "test_file_path =loadFile('Testing Set')\n",
    "# print(train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0bb3fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Progressing：:   0%|▏                                                            | 1/462 [00:00<00:46,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Extract train Set donor signals**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Progressing：: 100%|██████████████████████████████████████████████████████████| 462/462 [00:01<00:00, 245.82it/s]\n",
      "test Progressing：:   6%|███▎                                                        | 32/570 [00:00<00:03, 166.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Dataset Train info Finished!\n",
      "**********Extract test Set donor signals**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test Progressing：: 100%|███████████████████████████████████████████████████████████| 570/570 [00:01<00:00, 339.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Dataset Test info Finished!\n"
     ]
    }
   ],
   "source": [
    "def extract_donor_signal(file_path,dataset):\n",
    "    '''\n",
    "    Parameter：train_file_path|test_file_path [set_dataset]\n",
    "    Output:file_donor_positions,file_acceptor_positions,file_donor_signals,donor_signal_all\n",
    "    '''\n",
    "    print(f'Extract {dataset} Set donor signals'.center(50, '*'))\n",
    "    donor_positions= [] #1\n",
    "    acceptor_positions= [] #1\n",
    "    donor_signals=[]  #1\n",
    "    acceptor_signals=[]   #1\n",
    "    all_donor_signal=[]  \n",
    "    all_acceptor_signal=[]\n",
    "    length_list = [] #1\n",
    "    seq_list = [] #1\n",
    "    exons = [] #1\n",
    "    locus =[]\n",
    "    donor_file = []\n",
    "    files = tqdm(file_path, desc=f'{dataset} Progressing：')\n",
    "    for file in files:\n",
    "        f = open(file)\n",
    "        #  first line: extract gene locus\n",
    "        first_line =f.readline() \n",
    "        if dataset == \"test\":\n",
    "            locus.append(re.search(\">(.+)$\",first_line).group(1))\n",
    "        elif dataset==\"train\":\n",
    "            locus.append(first_line.split()[1]) \n",
    "        #  second line: extract  donor and acceptor site positions\n",
    "        second_line=f.readline()  \n",
    "        exon_positions_list = re.findall(r'(\\d+)\\.\\.(\\d+)',second_line)\n",
    "        donor_positions_list = [int(pos_set[1])+1 for pos_set in exon_positions_list[:-1]]\n",
    "        acceptor_positions_list= [int(pos_set[0])-1 for pos_set in exon_positions_list[1:]]\n",
    "        exons.append(exon_positions_list)\n",
    "        donor_positions.append(donor_positions_list)\n",
    "        acceptor_positions.append(acceptor_positions_list)\n",
    "        seq = ''\n",
    "        # extract  seq info\n",
    "        for line in f.readlines():\n",
    "            seq += line.strip()\n",
    "        seq_length = len(seq)\n",
    "        seq_list.append(seq.lower())\n",
    "        length_list.append(seq_length)\n",
    "        # extract  donor site signal\n",
    "        donor_signal=[]\n",
    "        for pos in donor_positions_list:\n",
    "            signal_range = seq[pos-1-left:pos-1+right].lower()\n",
    "            donor_signal.append(signal_range)\n",
    "            all_donor_signal.append(signal_range)\n",
    "            donor_file.append(file)\n",
    "        donor_signals.append(donor_signal)\n",
    "        # extract  acceptor site signal\n",
    "        acceptor_signal=[]\n",
    "        for pos in acceptor_positions_list:\n",
    "            signal_range = seq[pos-right:pos+left].lower()\n",
    "            acceptor_signal.append(signal_range)\n",
    "            all_acceptor_signal.append(signal_range)\n",
    "        acceptor_signals.append(acceptor_signal)\n",
    "    df_set_info = pd.DataFrame({'Path':file_path, 'Locus':locus,\"Length\":length_list,\"Exon Num\":[ len(exons) for exons  in exons],\\\n",
    " \"Exon Location\":exons,\"Donor Site\":donor_positions,\"Acceptor Site\":acceptor_positions,\"Donor signals\":donor_signals,\"Acceptor signals\":acceptor_signals})\n",
    "    df_set_info.to_csv(f'SVMoutput/{dataset.capitalize()}_set_info(non-seq).csv',index=None)\n",
    "    np.savetxt(f'SVMoutput/{dataset.capitalize()}_seq_list.txt',seq_list,delimiter = ',',fmt='%s')\n",
    "    np.savetxt(f'SVMoutput/{dataset.capitalize()}_donor_signal.txt',all_donor_signal,delimiter = ',',fmt='%s')\n",
    "    print(f\"Extract Dataset {dataset.capitalize()} info Finished!\")\n",
    "    if dataset==\"train\":\n",
    "        return all_donor_signal,all_acceptor_signal,seq_list,donor_positions,acceptor_positions\n",
    "    elif dataset==\"test\":\n",
    "        return all_donor_signal,all_acceptor_signal,seq_list,donor_positions,acceptor_positions,donor_file\n",
    "\n",
    "train_donor_signal_all_str,train_acceptor_signal_all_str,train_seq_list,\\\n",
    "train_donor_positions,train_acceptor_positions=extract_donor_signal(train_file_path,dataset=\"train\")\n",
    "test_donor_signal_all_str,test_acceptor_signal_all_str,test_seq_list,\\\n",
    "test_donor_positions,test_acceptor_positions,test_donor_filepath=extract_donor_signal(test_file_path,dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3ec0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "test_donor_positions_1d = reduce(operator.add,test_donor_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ad1cd8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save  donor str in Training Setting\n",
    "# y = np.loadtxt('SVMoutput/donor_signal.txt',delimiter = ',',dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84196b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert a DNA sequence string to a numpy array\n",
    "# converts to lower case, changes any non 'acgt' characters to 'n'\n",
    "import numpy as np\n",
    "import re\n",
    "sub_pattern = re.compile('[^acgt]')\n",
    "def string_to_array(my_string):\n",
    "    \"\"\"\n",
    "    function to convert a DNA sequence string to a numpy array\n",
    "    converts to lower case, changes any non 'acgt' characters to 'n'\n",
    "    like: ['c' 'a' 't' 'g' 'g']\n",
    "    \"\"\"\n",
    "    #my_string = my_string.lower()\n",
    "    sub_string = sub_pattern.sub('z', my_string)\n",
    "    return list(sub_string)\n",
    "\n",
    "def one_hot_encoder(s):\n",
    "    \"\"\"\n",
    "    return onehot code，try 5d?\n",
    "    \"\"\"\n",
    "    code_dict={\"a\": [1,0,0,0] ,\"g\":[0,1,0,0],\"c\":[0,0,1,0],\"t\":[0,0,0,1],\"z\":[0,0,0,0]}\n",
    "    one_hot = []\n",
    "    for i in s:\n",
    "        one_hot += code_dict[i] #2d\n",
    "    return one_hot\n",
    "def code_all_seq(seqs):\n",
    "    seqs = tqdm(seqs, desc='code_all_seq:')\n",
    "    return np.array(list(map(one_hot_encoder, map(string_to_array,seqs))))\n",
    "# create a label encoder with 'acgtn' alphabet\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(np.array(['a','c','g','t','z']))\n",
    "# function to encode a DNA sequence string as an ordinal vector\n",
    "# returns a numpy vector with a=0.25, c=0.50, g=0.75, t=1.00, n=0.00\n",
    "# def ordinal_encoder(my_array):\n",
    "#     integer_encoded = label_encoder.transform(my_array)\n",
    "#     float_encoded = integer_encoded.astype(float)\n",
    "#     float_encoded[float_encoded == 0] = 0.25 # A\n",
    "#     float_encoded[float_encoded == 1] = 0.50 # C\n",
    "#     float_encoded[float_encoded == 2] = 0.75 # G\n",
    "#     float_encoded[float_encoded == 3] = 1.00 # T\n",
    "#     float_encoded[float_encoded == 4] = 0.00 # anything else, z\n",
    "#     return float_encoded\n",
    "# test_sequence = 'aaccggtn'\n",
    "# ordinal_encoder(string_to_array(test_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c00db201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试编码效果\n",
    "test_sequence = 'aaccggtn'\n",
    "np.array(one_hot_encoder(string_to_array(test_sequence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bfc5042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2380"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_donor_signal_all_str) # 2381\n",
    "len(set(train_donor_signal_all_str)) # 716对，不去重了，毕竟正样本都没多少，你还去重！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b8e3a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones([1,10],dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51f7673c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donor = \"agtgtaagt\"\n",
    "donor[left:left+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db5dfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Non Donor Signal Sequence:: 100%|██████████████████████████████████████████| 462/462 [00:02<00:00, 177.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Non Donor Signal Sequence successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(\"[^acgt]\")\n",
    "def create_pseudoDonor(file_path,seqs_DNA, donor_locations,dataset,ran_num=0):\n",
    "    '''\n",
    "    output :pseudo donor signal containing 'gt' in the right position\n",
    "    要考虑acceptor附近的序列吗\n",
    "    '''\n",
    "    nonDonors = []\n",
    "    nonDonor_file_path = []\n",
    "    nonDonor_positions = []\n",
    "    file_num = tqdm(range(len(donor_locations)), desc='Creating Non Donor Signal Sequence:')\n",
    "    for i in file_num:\n",
    "        # 每个文件循环\n",
    "        file_nonDonors= []\n",
    "        file_seq_DNA = seqs_DNA[i]\n",
    "        num = len(donor_locations[i])  \n",
    "        length = len(file_seq_DNA)\n",
    "        donor_signals_start=[pos-1-left for pos in donor_locations[i]]\n",
    "#         acceptor_signals_start=[pos-right for pos in acceptor_locations[i] ]\n",
    "#         signals_start=sorted(donor_signals_start+acceptor_signals_start)\n",
    "        for index in range(length-signal_num+1):\n",
    "            if (file_seq_DNA[index+left:index+left+2] =='gt' ) and (index not in donor_signals_start) :\n",
    "                nonDonor = file_seq_DNA[index:index + signal_num]\n",
    "                #                 no_known =pattern.search(nonDonor) # 暂时先把未知的位点去掉\n",
    "#                 no_known =pattern.search(nonDonor)\n",
    "#                 if no_known:\n",
    "#                     continue # 这里之前写成break，有问题，这样遇到非正常碱基对的就直接循环中停止了\n",
    "                file_nonDonors.append(nonDonor)\n",
    "                nonDonor_file_path.append(file_path[i])\n",
    "                nonDonor_positions.append(index+1+left)\n",
    "        if ran_num:\n",
    "            nonDonors += random.sample(file_nonDonors,ran_num)\n",
    "        else:\n",
    "            nonDonors += file_nonDonors\n",
    "#         nonDonors.append(random.sample(file_nonDonors,ran_num))\n",
    "    print('Created Non Donor Signal Sequence successful!')\n",
    "    if dataset == \"train\":\n",
    "         return nonDonors\n",
    "    elif dataset == \"test\":\n",
    "        return nonDonors,nonDonor_file_path,nonDonor_positions\n",
    "\n",
    "# 训练集\n",
    "train_nonDonor_list=create_pseudoDonor(train_file_path,train_seq_list, train_donor_positions,dataset = \"train\")\n",
    "# print(test_nonDonor_list[1:10])\n",
    "train_nonDonor_len= len(train_nonDonor_list) \n",
    "# print(train_nonDonor_len) # #5518899 - > 283780 -> 283607 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d8d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a44ff6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Non Donor Signal Sequence:: 100%|██████████████████████████████████████████| 462/462 [00:02<00:00, 202.55it/s]\n",
      "code_all_seq:: 100%|████████████████████████████████████████████████████████████| 2381/2381 [00:00<00:00, 91826.09it/s]\n",
      "code_all_seq:: 100%|████████████████████████████████████████████████████████████| 2079/2079 [00:00<00:00, 99255.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Non Donor Signal Sequence successful!\n"
     ]
    }
   ],
   "source": [
    "# train_nonDonor_uniq = set(train_nonDonor_list)\n",
    "# train_nonDonor_uniq_len=len(train_nonDonor_uniq) \n",
    "# print(train_nonDonor_uniq_len)\n",
    "# Training Dataset extract nonDonor signal\n",
    "train_nonDonor_list=create_pseudoDonor(train_file_path,train_seq_list, train_donor_positions,dataset = \"train\")\n",
    "train_nonDonor_len= len(train_nonDonor_list) \n",
    "train_donor_features = code_all_seq(train_donor_signal_all_str)\n",
    "train_labels=[1]*len(train_donor_signal_all_str)\n",
    "\n",
    "train_nonDonor_ran_len =int(2079)\n",
    "random.seed(1)\n",
    "train_nonDonor_list =random.sample(train_nonDonor_list,train_nonDonor_ran_len)\n",
    "\n",
    "# Testing Dataset donor code\n",
    "train_nonDonor_features = code_all_seq(train_nonDonor_list)\n",
    "train_labels += [0]*train_nonDonor_ran_len\n",
    "train_labels = np.array(train_labels) # lenth:286161\n",
    "train_features =np.vstack([train_donor_features,train_nonDonor_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a22a432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Non Donor Signal Sequence:: 100%|██████████████████████████████████████████| 570/570 [00:01<00:00, 449.05it/s]\n",
      "code_all_seq:: 100%|████████████████████████████████████████████████████████████| 2079/2079 [00:00<00:00, 41692.17it/s]\n",
      "code_all_seq::   5%|███                                                       | 7913/148357 [00:00<00:01, 78556.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Non Donor Signal Sequence successful!\n",
      "148357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code_all_seq:: 100%|████████████████████████████████████████████████████████| 148357/148357 [00:02<00:00, 63514.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150436, 160)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing Dataset extract nonDonor signal\n",
    "\n",
    "test_nonDonor_list,test_nonDonor_filepath,test_nonDonor_positions=create_pseudoDonor(test_file_path,test_seq_list,\\\n",
    "                                                                                      test_donor_positions,dataset = \"test\")\n",
    "test_nonDonor_len= len(test_nonDonor_list) \n",
    "print(len(test_nonDonor_list)) # 149165\n",
    "\n",
    "# Testing Dataset donor code\n",
    "test_donor_features = code_all_seq(test_donor_signal_all_str)\n",
    "test_labels=[1]*len(test_donor_signal_all_str)\n",
    "\n",
    "test_nonDonor_features = code_all_seq(test_nonDonor_list)\n",
    "test_labels += [0]*test_nonDonor_len\n",
    "test_labels = np.array(test_labels)\n",
    "test_features =np.vstack([test_donor_features,test_nonDonor_features])\n",
    "print(test_features.shape) # 151244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "813f05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\"Filename\":test_donor_filepath+test_nonDonor_filepath,\"Donor Site\":test_donor_positions_1d +test_nonDonor_positions,\\\n",
    "              \"Signal\":test_donor_signal_all_str+test_nonDonor_list,\"label\":test_labels})\n",
    "test_df.to_csv(\"SVMoutput/Test_predict.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7803f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.metrics import classification_report\n",
    "# def save_to_file(file_name, contents,mode='a+'):\n",
    "#     fh = open(file_name, mode)\n",
    "#     fh.write(contents)\n",
    "#     fh.close()\n",
    "# # param_grid = {'C':[1,2,3,4,5,6,10]}#设置的C可能的值是1，2，5，10，可以自由设置\n",
    "# param_grid = [\n",
    "# #   {'C': [1, 10, 100, 1000], 'kernel': ['linear'],'class_weight':[{0:1000000,1:1},{0:100000,1:1},{0:10000,1:1},{0:1000,1:1},{0:100,1:1}, {0:10,1:1},'balanced']},\n",
    "# #   {'C': [0.1,1, 10, 100, 1000], 'gamma': [0.1,0.01,0.001, 0.0001,'auto','scale'], 'kernel': ['rbf'],'class_weight':[{0:1,1:1000000},{0:1,1:100000},{0:1,1:10000},{0:1,1:1000},{0:1,1:100}, {0:1,1:10},'balanced']},\n",
    "#     {'C': [0.1,1], 'gamma': [1,'auto','scale'], 'kernel': ['rbf'],'class_weight':[{0:1,1:100}, {0:1,1:10},'balanced']}\n",
    "# ]\n",
    "# svc = SVC(probability=True)\n",
    "\n",
    "# # define evaluation procedure\n",
    "# # cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# cv = [(slice(None), slice(None))] #去掉十成交叉\n",
    "# # cv = 3\n",
    "# grid = GridSearchCV(svc,param_grid,cv=cv,verbose=3,n_jobs=-1,scoring='f1')\n",
    "# grid_result=grid.fit(train_features, train_labels)\n",
    "\n",
    "# # report the best configuration\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# save_to_file('SVMoutput/SVM_best_predict.txt', \"Best: %f using %s \\n\" % (grid_result.best_score_, grid_result.best_params_),mode='w+') \n",
    "\n",
    "# # report all configurations\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# model=grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cee5cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# joblib.dump(model,'SVMoutput/model.pickle') #保存\n",
    "# # model = joblib.load('SVMoutput/model.pickle') #载入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e7d8d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_predict = model.predict(train_features)\n",
    "# report =classification_report(train_labels, train_predict)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3cd7976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4460"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80175b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SVC(kernel='rbf',C=1,gamma=1,class_weight='balanced',probability=True)\n",
    "# model.fit(train_features, train_labels)\n",
    "# train_predict = model.predict(train_features)\n",
    "# report =classification_report(train_labels, train_predict)\n",
    "\n",
    "# print(report)\n",
    "model = SVC(kernel='rbf',C=0.1,gamma=1,probability=True)\n",
    "model.fit(test_features, test_labels)\n",
    "# train_predict = model.predict(test_features)\n",
    "# report =classification_report(test_labels, test_predict)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedaa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predict = model.predict(test_features)\n",
    "# report_dict =classification_report(test_labels, test_predict,target_names=['pseudo donor','real donor'],digits=4,output_dict=True)\n",
    "# report_df = pd.DataFrame(report_dict).T\n",
    "# report_df.to_csv(\"SVMoutput/SVM_report.csv\")\n",
    "pred = model.predict_proba(test_features)\n",
    "# report_df\n",
    "pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47945292-3617-407c-8651-a94bb5f8a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f04a0-48bb-4888-bbb0-6c63fa2be1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SVMoutput/SVM_predict_scores.csv\")\n",
    "df['RBF']=pred[:,1]\n",
    "df.to_csv(\"SVMoutput/SVM_predict_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3601aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report_dict =classification_report(test_labels, test_predict,target_names=['pseudo donor','real donor'],digits=4,output_dict=True)\n",
    "# report_df = pd.DataFrame(report_dict).T\n",
    "# report_df.to_csv(\"SVMoutput/SVM_report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19059584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notify_run import Notify\n",
    "# n = Notify()\n",
    "# n.send(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d23bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predict info\n",
    "# test_df[\"Score\"] = pred[:,1]\n",
    "# test_df[\"predict\"] = test_predict\n",
    "# test_df = test_df.sort_values(by=['Filename','label'],ascending=[True, False]).reset_index(drop=True)\n",
    "# test_df.to_csv(\"SVMoutput/Test_predict.csv\",index=None)\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8625525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l = SVC(kernel='linear',C=1,class_weight='balanced',probability=True)\n",
    "model_l.fit(train_features, train_labels)\n",
    "# train_predict_l = model_l.predict(train_features)\n",
    "# print(classification_report(train_labels, train_predict_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_l = model_l.predict(test_features)\n",
    "report_dict_l =classification_report(test_labels, test_predict_l,target_names=['pseudo donor','real donor'],digits=4,output_dict=True)\n",
    "report_df_l = pd.DataFrame(report_dict_l).T\n",
    "report_df_l.to_csv(\"SVMoutput/SVM_report_l.csv\")\n",
    "pred_l = model_l.predict_proba(test_features)\n",
    "report_df_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.01: real donor\t0.146730\t0.960558\t0.254573\t2079.000000\n",
    "model_p = SVC(kernel='poly',C=1,degree=4,gamma='scale',class_weight='balanced',probability=True)\n",
    "model_p.fit(train_features, train_labels)\n",
    "# train_predict_p = model_p.predict(train_features)\n",
    "# print(classification_report(train_labels, train_predict_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_p = model_p.predict(test_features)\n",
    "report_dict_p =classification_report(test_labels, test_predict_p,target_names=['pseudo donor','real donor'],digits=4,output_dict=True)\n",
    "report_df_p = pd.DataFrame(report_dict_p).T\n",
    "report_df_p.to_csv(\"SVMoutput/SVM_report_p.csv\")\n",
    "pred_p = model_p.predict_proba(test_features)\n",
    "report_df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8517c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model,'SVMoutput/model_r.pickle') #save\n",
    "joblib.dump(model_l,'SVMoutput/model_l.pickle') \n",
    "joblib.dump(model_p,'SVMoutput/model_p.pickle') \n",
    "# model = joblib.load('SVMoutput/model.pickle') #load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e59387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_predict_scores = pd.DataFrame(data={\"RBF\":pred[:,1],\"Linear\":pred_l[:,1],\"Poly\":pred_p[:,1]})\n",
    "SVM_predict_scores.to_csv(\"SVMoutput/SVM_predict_scores.csv\",index=None)\n",
    "SVM_predict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405dc855",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probs = [pred[:,1],pred_l[:,1],pred_p[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC plot\n",
    "# problem ： 如何画不同核的图在一个图里\n",
    "from sklearn.metrics import roc_curve, auc  ###计算roc和auc\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib as mpl\n",
    "# def Find_Optimal_Cutoff(TPR, FPR, threshold):\n",
    "#     y = TPR - FPR\n",
    "#     Youden_index = np.argmax(y)  # Only the first occurrence is returned.\n",
    "#     optimal_threshold = threshold[Youden_index]\n",
    "#     point = [FPR[Youden_index], TPR[Youden_index]]\n",
    "#     return optimal_threshold, point\n",
    "# def ROC(label, y_prob):\n",
    "#     \"\"\"\n",
    "#     Receiver_Operating_Characteristic, ROC\n",
    "#     :param label: (n, )\n",
    "#     :param y_prob: (n, )\n",
    "#     :return: fpr, tpr, roc_auc, optimal_th, optimal_point\n",
    "#     \"\"\"\n",
    "#     fpr, tpr, thresholds = roc_curve(label, y_prob)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     optimal_th, optimal_point = Find_Optimal_Cutoff(TPR=tpr, FPR=fpr, threshold=thresholds)\n",
    "#     return fpr, tpr, roc_auc, optimal_th, optimal_point\n",
    "# def ROC_plot(label, y_prob):\n",
    "#     fpr, tpr, roc_auc, optimal_th, optimal_point = ROC(label, y_prob)\n",
    "#     plt.figure()\n",
    "#     plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "#     plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "#     plt.plot(optimal_point[0], optimal_point[1], marker='o', color='r')\n",
    "#     plt.text(optimal_point[0], optimal_point[1], f'Threshold:{optimal_th:.2f}')\n",
    "#     plt.title(\"Receiver Operating Characteristic Curve\")\n",
    "#     plt.xlabel(\"False Positive Rate\")\n",
    "#     plt.ylabel(\"True Positive Rate\")\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     f = plt.gcf()  #获取当前图像\n",
    "#     f.savefig('SVMoutput/ROC_plot.png',dpi=120)\n",
    "#     plt.show()\n",
    "#     return optimal_th\n",
    "# pred  = model.decision_function(test_features)\n",
    "\n",
    "\n",
    "def plot_roc(labels, predict_probs, titles):\n",
    "    color = ['r', 'g', 'b', 'y']                                                                 \n",
    "    linestyles = ['-','--',':','-.']\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    \n",
    "    for idx, predict_prob in enumerate(predict_probs):\n",
    "        false_positive_rate,true_positive_rate,thresholds=roc_curve(labels, predict_prob)\n",
    "        roc_auc=auc(false_positive_rate, true_positive_rate)\n",
    "        c = color[idx%len(color)]                                                                     \n",
    "        l =linestyles[idx%len(linestyles)]\n",
    "        ax.plot(false_positive_rate, true_positive_rate,'b',label='AUC: {} = {:.4}'.format(titles[idx], roc_auc), color=c, linestyle=l, alpha=0.9,markevery=20)  \n",
    "        ax.legend(loc='lower right')\n",
    "    # \n",
    "    axins = inset_axes(ax, width=\"40%\", height=\"30%\",loc='upper center',\n",
    "                   bbox_to_anchor=(0.05, -0.2, 1, 1),\n",
    "                   bbox_transform=ax.transAxes)\n",
    "    for idx, predict_prob in enumerate(predict_probs):\n",
    "        false_positive_rate,true_positive_rate,thresholds=roc_curve(labels, predict_prob)\n",
    "        roc_auc=auc(false_positive_rate, true_positive_rate)\n",
    "        c = color[idx%len(color)]                                                                     \n",
    "        l =linestyles[idx%len(linestyles)]\n",
    "        axins.plot(false_positive_rate, true_positive_rate,'b',label='AUC: {} = {:.4}'.format(titles[idx], roc_auc), color=c, linestyle=l, alpha=0.9,markevery=20)  \n",
    "\n",
    "    axins.set_xlim(0.03, 0.13)\n",
    "    axins.set_ylim(0.89, 0.99)\n",
    "    #\n",
    "    mark_inset(ax, axins, loc1=3,loc2=1, fc=\"none\", ec='k', lw=1)\n",
    "    ax.plot([0,1],[0,1],'k--',alpha=0.5)\n",
    "    ax.set_title(\"Receiver Operating Characteristic Curve\",fontsize=16)\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    ax.spines['left'].set_color('k')\n",
    "    [axins.spines[loc_axis].set_color('k') for loc_axis in ['top','right','bottom','left']]\n",
    "    plt.savefig('SVMoutput/ROC_plot.png',dpi=120)\n",
    "    plt.show()\n",
    "\n",
    "# plt.style.use('classic')\n",
    "\n",
    "# mpl.style.use('fivethirtyeight')\n",
    "# mpl.style.use('classic')\n",
    "mpl.style.use('default')\n",
    "\n",
    "# sns.set_style('darkgrid', {'axes.linewidth': 2, 'axes.edgecolor':'black'})\n",
    "# rc = {\"axes.spines.bottom\" : False,\n",
    "#       \"axes.spines.top\"    : False,\n",
    "#       \"axes.spines.right\"  : False,\n",
    "#       \"axes.edgecolor\"     : \"black\"}\n",
    "# plt.style.use((\"ggplot\", rc))\n",
    "# plt.style.use((\"ggplot\"))\n",
    "# mpl.style.use((\"fast\"))\n",
    "predict_probs = [pred[:,1],pred_l[:,1],pred_p[:,1]]\n",
    "plot_roc(test_labels, predict_probs,titles=[\"RBF\",\"Linear\",\"Poly\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb811d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "def plot_pr(labels, predict_probs, titles):\n",
    "    color = ['r', 'g', 'b', 'y']                                                                 \n",
    "    linestyles = ['-','--','-.']\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        l, = plt.plot(x[y >= 0], y[y >= 0], color='gray',linestyle='--' ,alpha=0.2)\n",
    "        plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    for idx, predict_prob in enumerate(predict_probs):\n",
    "        precision, recall, thresholds = precision_recall_curve(labels, predict_prob)\n",
    "        pr_auc=auc(recall, precision)\n",
    "        c = color[idx%len(color)]                                                                     \n",
    "\n",
    "        l =linestyles[idx%len(linestyles)]\n",
    "        plt.plot(recall,precision,'b', label='AUC: {} = {:.4}'.format(titles[idx], pr_auc),color=c, linestyle=l, alpha=0.9,markevery=20)  \n",
    "        plt.legend(loc='lower right')\n",
    "    plt.ylim(0,1)\n",
    "    # plt.plot([0,1],[1,0],color='gray',linestyle='--',alpha=0.5)\n",
    "    plt.title('Precision/Recall Curve',fontsize=16)# give plot a title\n",
    "    plt.xlabel('Recall')# make axis labels\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    f = plt.gcf()  \n",
    "    f.savefig('SVMoutput/PR_plot.png',dpi=120)\n",
    "    plt.show()\n",
    "\n",
    "plot_pr(test_labels, predict_probs, titles=[\"RBF\",\"Linear\",\"Poly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混淆矩阵,，\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def save_to_file(file_name, contents,mode='a+'):\n",
    "    fh = open(file_name, mode)\n",
    "    fh.write(contents)\n",
    "    fh.close()\n",
    "def draw_confusion_matrix(labels,predicts,name,size=(6, 6),mode='a+',threshold=\"Default\"):\n",
    "    mat = confusion_matrix(labels,predicts)\n",
    "    TP = mat[1][1]\n",
    "    FP = mat[0][1]\n",
    "    TN = mat[0][0]\n",
    "    FN = mat[1][0]\n",
    "    recall  = TP/(FN+TP)\n",
    "    precision = TP/(FP+TP)\n",
    "    acc = (TP+TN)/np.sum(mat)\n",
    "    fpr =  FP/(TN+ FP)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    print(f\"Precision :{precision}, Recall:{recall},f1_score:{f1_score},Acc:{acc},FPR:{fpr},Sp:{1-fpr}\")\n",
    "    save_to_file('SVMoutput/SVM_predict.txt', f\"\\nPrecision :{precision}, Recall:{recall},f1_score:{f1_score},Acc:{acc} \\n\",mode)\n",
    "\n",
    "    f, ax = plt.subplots(figsize=size)\n",
    "    sns.set()\n",
    "    sns.heatmap(mat, square=True, annot=True, fmt='d',cmap='Blues' ,cbar=False,\n",
    "                xticklabels=['pseudo donor','real donor'],\n",
    "                yticklabels=['pseudo donor','real donor'],)\n",
    "    ax.set_xlabel(f'Predicted label\\nPrecision :{precision:.2f}, Recall:{recall:.2f},F1_score:{f1_score:.2f}')\n",
    "    ax.set_ylabel(f'True label');\n",
    "    plt.title(f\"Confusion Matrix(Threshold:{threshold})\\n\",fontsize=15)\n",
    "    f.savefig(f\"SVMoutput/{name}.png\",dpi=120)\n",
    "\n",
    "draw_confusion_matrix(test_labels, test_predict,name=\"confusion_matrix_1\",mode='w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predict_label =[1 if i >0.65 else 0 for i in pred_p[:,1] ]\n",
    "draw_confusion_matrix(test_labels, best_predict_label,name=\"confusion_matrix_p_0.65\",threshold=0.65)\n",
    "best_predict_label =[1 if i >0.07 else 0 for i in pred_p[:,1] ]\n",
    "draw_confusion_matrix(test_labels, best_predict_label,name=\"confusion_matrix_p_0.07\",threshold=0.07)\n",
    "best_predict_label =[1 if i >0.2 else 0 for i in pred_p[:,1] ]\n",
    "draw_confusion_matrix(test_labels, best_predict_label,name=\"confusion_matrix_p_0.2\",threshold=0.2)\n",
    "best_predict_label =[1 if i >0.9 else 0 for i in pred_p[:,1] ]\n",
    "draw_confusion_matrix(test_labels, best_predict_label,name=\"confusion_matrix_p_0.9\",threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果根据阈值来重新生成混淆矩阵？\n",
    "best_predict_label =[1 if i >0.07 else 0 for i in pred[:,1] ]\n",
    "draw_confusion_matrix(test_labels, best_predict_label,name=\"confusion_matrix_0.07\",threshold=0.07)\n",
    "best_predict_label =[1 if i >0.2 else 0 for i in pred[:,1] ]\n",
    "draw_confusion_matrix(test_labels, best_predict_label,name=\"confusion_matrix_0.2\",threshold=0.2)\n",
    "best_predict_label =[1 if i >0.9 else 0 for i in pred[:,1] ]\n",
    "draw_confusion_matrix(test_labels, best_predict_label,name=\"confusion_matrix_0.9\",threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309cabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_threshold_info(test_labels,pred):\n",
    "    TPs  = []\n",
    "    FPs = []\n",
    "    TNs = []\n",
    "    FNs = []\n",
    "    for threshold in thresholds:\n",
    "        predict_label =[1 if i >= threshold else 0 for i in pred ]\n",
    "        mat = confusion_matrix(test_labels,predict_label)\n",
    "        TP = mat[1][1]\n",
    "        FP = mat[0][1]\n",
    "        TN = mat[0][0]\n",
    "        FN = mat[1][0]\n",
    "\n",
    "        TPs.append(TP)\n",
    "        FPs.append(FP)\n",
    "        TNs.append(TN)\n",
    "        FNs.append(FN)\n",
    "    SVMpredict_df = pd.DataFrame({\"Threshold\":thresholds,\"TP\":TPs,\"FP\":FPs,\"TN\":TNs,\"FN\":FNs})\n",
    "    SVMpredict_df[\"Recall\"] = SVMpredict_df[\"TP\"] /(SVMpredict_df[\"TP\"]+SVMpredict_df[\"FN\"])\n",
    "    SVMpredict_df[\"Precision\"] = SVMpredict_df[\"TP\"] /(SVMpredict_df[\"TP\"]+SVMpredict_df[\"FP\"])\n",
    "    SVMpredict_df[\"F1-Score\"] = 2*SVMpredict_df[\"Recall\"]*SVMpredict_df[\"Precision\"] /(SVMpredict_df[\"Recall\"]+SVMpredict_df[\"Precision\"])\n",
    "    SVMpredict_df[\"Sn\"] = SVMpredict_df[\"TN\"] /(SVMpredict_df[\"TN\"]+SVMpredict_df[\"FP\"])\n",
    "    SVMpredict_df[\"FPR\"] =  1- SVMpredict_df[\"Sn\"]\n",
    "    SVMpredict_df[\"Acc\"] =  (SVMpredict_df[\"TP\"] + SVMpredict_df[\"TN\"]) / (SVMpredict_df[\"TP\"] + SVMpredict_df[\"TN\"]+SVMpredict_df[\"FP\"] + SVMpredict_df[\"FN\"])\n",
    "    return SVMpredict_df\n",
    "def plot_PRF(SVMpredict_df):\n",
    "    f, ax = plt.subplots(figsize=(8,8))\n",
    "    alpha=0.5\n",
    "    plt.plot(thresholds, SVMpredict_df[\"Recall\"], color='green', label='Recall',alpha=alpha)\n",
    "    plt.plot(thresholds, SVMpredict_df[\"Precision\"], color='red', label='Precison',alpha=alpha)\n",
    "    plt.plot(thresholds, SVMpredict_df[\"F1-Score\"], color='#2196f3', label='F1-score',alpha=alpha)\n",
    "    plt.xlabel(\"Thresholds\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(np.arange(0,1.1,0.1))\n",
    "    plt.yticks(np.arange(0,1.1,0.1))\n",
    "    plt.xlim([-0.05,0.9])\n",
    "    plt.legend(loc=\"lower right\") # 图标在外侧\n",
    "    plt.grid(linestyle='-.',alpha=0.7)\n",
    "    plt.show()\n",
    "    return f\n",
    "thresholds = list(np.arange(0,1.05,0.05)) \n",
    "SVMpredict_df_r=sum_threshold_info(test_labels,pred[:,1])\n",
    "SVMpredict_df_r.to_csv(\"SVMoutput/SVM_predict_info_r.csv\",index=None)\n",
    "f = plot_PRF(SVMpredict_df_r)\n",
    "f.savefig(\"SVMoutput/Different Threshold Predictions_r.png\",dpi=400,bbox_inches = 'tight')\n",
    "\n",
    "SVMpredict_df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78b266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f884c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMpredict_df_p=sum_threshold_info(test_labels,pred_p[:,1])\n",
    "SVMpredict_df_p.to_csv(\"SVMoutput/SVM_predict_info_p.csv\",index=None)\n",
    "f = plot_PRF(SVMpredict_df_p)\n",
    "f.savefig(\"SVMoutput/Different Threshold Predictions_p.png\",dpi=400,bbox_inches = 'tight')\n",
    "# sns.lineplot(data=[SVMpredict_df[\"Recall\"],SVMpredict_df[\"Precision\"],SVMpredict_df[\"F1-Score\"]],palette=\"tab10\",ax=ax)\n",
    "\n",
    "SVMpredict_df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33ea6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf336afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notify_run import Notify\n",
    "n = Notify()\n",
    "n.send(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c517c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aaf839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
