{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install watermark\n",
    "# !pip install seaborn\n",
    "# !pip install biopython\n",
    "# !pip install sklearn\n",
    "import os\n",
    "import re \n",
    "# from Bio import SeqIO\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import watermark\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls\n",
    "# !unzip 'Training and testing datasets.zip'  -d './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2021-04-30T09:44:38.292813+08:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.7.3\n",
      "IPython version      : 7.5.0\n",
      "\n",
      "Compiler    : GCC 7.3.0\n",
      "OS          : Linux\n",
      "Release     : 4.18.0-80.7.1.el8_0.x86_64\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 52\n",
      "Architecture: 64bit\n",
      "\n",
      "numpy    : 1.20.2\n",
      "watermark: 2.2.0\n",
      "pandas   : 1.2.4\n",
      "re       : 2.2.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDATA CLEAN\\ntrain set contains 462 files, all suffies of file is \"TXT\",the letters are  lowercase  letters.\\ntest set contains 570 files, the suffixes of file contains \"TXT\",\"txt\",and the letters are capital letters.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "DATA CLEAN\n",
    "train set contains 462 files,2831 introns, all suffies of file is \"TXT\",the letters are  lowercase  letters.\n",
    "test set contains 570 files, 2071 introns,the suffixes of file contains \"TXT\"and \"txt\",and the letters are capital letters.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "left=3\n",
    "right=6 \n",
    "signal_num = left+right\n",
    "bases=\"ACGT\"\n",
    "lower_bases=\"acgt\"\n",
    "# noknown_bases=\"nksrywmbv\" # manual counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(dirs):\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    else:\n",
    "        pass\n",
    "mkdir(\"output\")\n",
    "mkdir(\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOADING Training Set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 462/462 [00:00<00:00, 334386.27it/s]\n",
      "LOADING Testing Set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 571/571 [00:00<00:00, 406750.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462 Files Loading Finished!\n",
      "\n",
      "571 Files Loading Finished!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def loadFile(file_dir):\n",
    "    '''\n",
    "    Function: Read  All files in the Training Set Folder\n",
    "    Parameterï¼šfile_dir\n",
    "    Output: file_path,locus_list\n",
    "    Attention: do not load non-fasta files!\n",
    "    '''\n",
    "    file_path = []\n",
    "    file_locus_list = []\n",
    "    count=0\n",
    "    all_file = tqdm(os.listdir(file_dir), desc=f'LOADING {file_dir}')\n",
    "    for file_name in all_file:\n",
    "        count+=1\n",
    "        suffix = re.findall(\"\\.(.+$)\",file_name)[-1].lower()\n",
    "        # or  suffix = file_name.split(\".\")[1].lower()\n",
    "        if suffix != \"txt\":\n",
    "            continue\n",
    "        path = f\"{file_dir}/{file_name}\"\n",
    "        file_path.append(path)\n",
    "    all_file.write(f\"{count} Files Loading Finished!\")\n",
    "    all_file.close()\n",
    "    print()\n",
    "    return file_path,file_locus_list\n",
    "train_file_path,train_file_locus =loadFile('Training Set')\n",
    "test_file_path,test_file_locus =loadFile('Testing Set')\n",
    "# print('LOADINGÂ File'.center(50,'*'))\n",
    "# print(train_file_path)\n",
    "# print(len(train_file_locus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°è¯•ä½¿ç”¨bipythonçš„æ¨¡å—ï¼Œç„¶è€Œå¥½åƒä¸å¤ªå¥½ç”¨â€¦â€¦è¿˜æ˜¯ç”¨è‡ªå·±çš„å‡½æ•°å§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for seq_record in SeqIO.parse('Testing Set/ACU08131.TXT', \"fasta\"):\n",
    "#     print(seq_record.id)\n",
    "# #     print(type(seq_record.id))\n",
    "# #     print(seq_record.seq)\n",
    "#     print(str(seq_record.seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "åŸæ¥ä¸å¯¹åºåˆ—ç¼–ç ï¼Œå‘ç°è¿è¡Œé€Ÿåº¦è´¼æ…¢ï¼Œå¸Œæœ›ç”¨numpyé€Ÿåº¦ä¼šå¿«ç‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert a DNA sequence string to a numpy array\n",
    "# converts to lower case, changes any non 'acgt' characters to 'n'\n",
    "def string_to_array(my_string):\n",
    "    my_string = my_string.lower()\n",
    "    my_string = re.sub('[^acgt]', 'z', my_string)\n",
    "    my_array = np.array(list(my_string))\n",
    "    return my_array\n",
    "# function to encode a DNA sequence string as an ordinal vector\n",
    "# returns a numpy vector with a=0, c=1, g=2, t=3, n=4\n",
    "def ordinal_encoder(my_array):\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(np.array(['a','c','g','t','z']))\n",
    "    integer_encoded = label_encoder.transform(my_array)\n",
    "#     print(integer_encoded)\n",
    "#     float_encoded = integer_encoded.astype(float)\n",
    "#     float_encoded[float_encoded == 0] = 0.25 # A\n",
    "#     float_encoded[float_encoded == 1] = 0.50 # C\n",
    "#     float_encoded[float_encoded == 2] = 0.75 # G\n",
    "#     float_encoded[float_encoded == 3] = 1.00 # T\n",
    "#     float_encoded[float_encoded == 4] = 0.00 # anything else, z\n",
    "    return integer_encoded\n",
    "def one_hot_encoder(my_array):\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(np.array(['a','c','g','t','z']))\n",
    "    integer_encoded = label_encoder.transform(my_array)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False, dtype=int)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "#     onehot_encoded = np.delete(onehot_encoded, -1, 1)\n",
    "    return onehot_encoded\n",
    "def getKmers(sequence, size):\n",
    "    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]\n",
    "x=getKmers(str(seq_record.seq), size=6)\n",
    "mySeq = 'CATGGCCATCCCCCCCCGAGCGGGGGGGGGG'\n",
    "mySeq2 = 'GATGGCCATCCCCGCCCGAGCGGGGGGGG'\n",
    "mySeq3 = 'CATGGCCATCCCCGCCCGAGCGGGCGGGG'\n",
    "for test_sequence in x:\n",
    "    float_encoded = ordinal_encoder(string_to_array(test_sequence))\n",
    "#     one_hot_matrix = one_hot_encoder(string_to_array(test_sequence))\n",
    "    print(float_encoded)\n",
    "#     print(one_hot_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Progressingï¼š:  32%|â–ˆâ–ˆâ–ˆâ–      | 148/462 [00:00<00:00, 1132.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Extract Train Set donor signals**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Progressingï¼š: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 462/462 [00:00<00:00, 1396.89it/s]\n",
      "Test Progressingï¼š:  33%|â–ˆâ–ˆâ–ˆâ–      | 188/570 [00:00<00:00, 1874.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n",
      "**********Extract Test Set donor signals**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Progressingï¼š: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 1812.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##### è¯»å–æ¯ä¸ªæ–‡ä»¶ï¼ŒæŠ½å–donor site å‰3å’Œå6å…±ä¹ä¸ªç¢±åŸº #####\n",
    "def extract_donor_signal(file_path,set_folder):\n",
    "    '''\n",
    "    Parameterï¼štrain_file_path|test_file_path[dir]\n",
    "    Output:file_donor_positions,file_acceptor_positions,file_donor_signals,donor_signal_all\n",
    "    '''\n",
    "    print(f'Extract {set_folder} Set donor signals'.center(50, '*'))\n",
    "    file_donor_positions= []\n",
    "    file_acceptor_positions= []\n",
    "    file_donor_signals=[]\n",
    "    file_acceptor_signals=[]\n",
    "    all_donor_signal_str=[]\n",
    "    all_acceptor_signal_str=[]\n",
    "    file_length_list = []\n",
    "    file_seq_list = []\n",
    "    file_exons = []\n",
    "    # positions=[]\n",
    "    # print(\"Progressingï¼š\",end='')\n",
    "    file_path = tqdm(file_path, desc=f'{set_folder} Progressingï¼š')\n",
    "    for file in file_path:\n",
    "        f = open(file)\n",
    "        _=f.readline() # pass the first line \n",
    "        second_line=f.readline()  \n",
    "        # extract  donor and acceptor site\n",
    "        donor_positions_list = re.findall(r'\\d+\\.\\.(\\d+)',second_line)[:-1] \n",
    "        donor_positions_list = [int(pos)+1 for pos in donor_positions_list] \n",
    "        acceptor_positions_list = re.findall(r'(\\d+)\\.\\.\\d+',second_line)[1:]\n",
    "        acceptor_positions_list = [int(pos)-1 for pos in acceptor_positions_list]\n",
    "        exon_positions_list = re.findall(r'(\\d+)\\.\\.(\\d+)',second_line)\n",
    "        file_exons.append(exon_positions_list)\n",
    "        file_donor_positions.append(donor_positions_list)\n",
    "        file_acceptor_positions.append(acceptor_positions_list)\n",
    "        seq = ''\n",
    "        for line in f.readlines():\n",
    "                seq += line.strip()\n",
    "        # extract  donor site signal\n",
    "        seq_length = len(seq)\n",
    "        file_seq_list.append(seq)\n",
    "        file_length_list.append(seq_length)\n",
    "        donor_signal=[]\n",
    "        for pos in donor_positions_list:\n",
    "            # print(pos)\n",
    "            if pos+right-1<=seq_length:\n",
    "                signal_range = seq[pos-1-left:pos-1+right].lower() # Lowercase letters\n",
    "            else:\n",
    "                continue\n",
    "            # seq_range = seq[pos-left:pos+right] # æœ‰4ä¸ªä½ç‚¹å°±æ˜¯æ­£å¥½åœ¨åºåˆ—æœ€åï¼Œ æ‰€ä»¥éœ€è¦å‰”é™¤\n",
    "            # donor_signal.append(list(seq_range))\n",
    "            donor_signal.append(signal_range)\n",
    "            all_donor_signal_str.append(signal_range)\n",
    "        file_donor_signals.append(donor_signal)\n",
    "        acceptor_signal=[]\n",
    "        for pos in acceptor_positions_list:\n",
    "            # print(pos)\n",
    "            if pos-right>=0:\n",
    "                signal_range = seq[pos-right:pos+left].lower() # Lowercase letters\n",
    "            else:\n",
    "                continue \n",
    "            acceptor_signal.append(signal_range)\n",
    "            all_acceptor_signal_str.append(signal_range)\n",
    "        file_acceptor_signals.append(acceptor_signal)\n",
    "    print(\"Finished!\")\n",
    "    return file_donor_positions,file_donor_signals,all_donor_signal_str,file_acceptor_positions,\\\n",
    "file_acceptor_signals,all_acceptor_signal_str,file_length_list,file_seq_list,file_exons\n",
    "\n",
    "#  str= \"CDSjoin(28199..28271,28881..28988,34291..34586)\"\n",
    "#  exon_positions_list = re.findall(r'(\\d+)\\.\\.(\\d+)',str)\n",
    "#  print(exon_positions_list)\n",
    "\n",
    "train_file_donor_positions,train_file_donor_signals,train_donor_signal_all_str,\\\n",
    "train_file_acceptor_positions,train_file_acceptor_signals,train_acceptor_signal_all_str,\\\n",
    "train_file_length_list,train_file_seq_list,train_file_exons=extract_donor_signal(train_file_path,dir=\"Train\")\n",
    "\n",
    "test_file_donor_positions,test_file_donor_signals,test_donor_signal_all_str,\\\n",
    "test_file_acceptor_positions,test_file_acceptor_signals,test_acceptor_signal_all_str,\\\n",
    "test_file_length_list,test_file_seq_list,test_file_exons=extract_donor_signal(test_file_path,dir=\"Test\")\n",
    "# print(train_file_donor_positions)\n",
    "# print(train_file_acceptor_positions)\n",
    "# print(train_file_donor_signals[0])\n",
    "# print(len(train_file_donor_signals)) # return the num of rows\n",
    "# print(train_donor_signal_all_str)\n",
    "# print(test_file_donor_positions)\n",
    "# print(test_file_acceptor_positions)\n",
    "# print(test_file_donor_signals[0])\n",
    "# print(len(test_file_donor_signals)) # return the num of rows\n",
    "# print(test_donor_signal_all_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting  Base Distribution:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 462/462 [00:00<00:00, 656.83it/s]\n",
      "Counting  Base Distribution:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2381/2381 [00:00<00:00, 652764.09it/s]\n",
      "Counting  Base Distribution:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 1583.46it/s]\n",
      "Counting  Base Distribution:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2079/2079 [00:00<00:00, 646401.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of all bases in training set\n",
      " {'c': 1282733, 't': 1498203, 'a': 1436966, 'g': 1306260, 'n': 682, 'k': 28, 'b': 4, 's': 27, 'm': 16, 'r': 15, 'v': 2, 'y': 26, 'w': 14}\n",
      "Distribution of donor site bases in training  set\n",
      " {'c': 2030, 'g': 8909, 't': 4709, 'a': 5781}\n",
      "Distribution of all bases in testing set\n",
      " {'G': 702343, 'A': 736600, 'T': 762900, 'C': 689433, 'N': 862, 'Y': 3, 'R': 5, 'B': 1, 'S': 1, 'K': 1}\n",
      "Distribution of donor site  bases in testing set\n",
      " {'c': 1691, 'a': 5114, 'g': 7730, 't': 4176}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# æ‰å‘ç°æ–‡æœ¬é‡Œä¸åªæ˜¯æœ‰ACGTï¼Œè¿˜æœ‰å…¶ä»–æ•°å€¼\n",
    "def count_each_char(str_list):\n",
    "    dict = {}\n",
    "    str_list = tqdm(str_list, desc='Counting  Base Distribution:')\n",
    "    for seq in str_list:\n",
    "         for i  in seq:\n",
    "            if i not in dict:\n",
    "                dict[i] = 1\n",
    "            else:\n",
    "                dict[i] += 1\n",
    "    return dict\n",
    "\n",
    "base_dis_1 = count_each_char(train_file_seq_list)\n",
    "base_dis_2 = count_each_char(train_donor_signal_all_str)\n",
    "base_dis_3 = count_each_char(test_file_seq_list)\n",
    "base_dis_4 = count_each_char(test_donor_signal_all_str)\n",
    "print(\"Distribution of all bases in training set\\n\",base_dis_1)\n",
    "print(\"Distribution of donor site bases in training  set\\n\",base_dis_2)\n",
    "print(\"Distribution of all bases in testing set\\n\",base_dis_3)\n",
    "print(\"Distribution of donor site  bases in testing set\\n\",base_dis_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -3 -2 -1  0  1  2  3  4  5\n",
      "0     c  g  g  g  t  a  t  g  t\n",
      "1     g  c  a  g  t  g  a  g  t\n",
      "2     a  a  t  g  t  a  a  g  t\n",
      "3     c  a  g  g  t  a  a  g  a\n",
      "4     a  a  g  g  t  g  a  g  t\n",
      "...  .. .. .. .. .. .. .. .. ..\n",
      "2376  a  a  g  g  t  a  a  g  a\n",
      "2377  c  a  g  g  t  g  a  g  a\n",
      "2378  a  a  g  g  t  a  a  g  g\n",
      "2379  c  a  g  g  t  g  a  g  g\n",
      "2380  t  c  g  g  t  g  a  g  t\n",
      "\n",
      "[2381 rows x 9 columns]\n",
      "     -5 -4 -3 -2 -1  0  1  2  3\n",
      "0     c  c  c  c  a  g  g  a  t\n",
      "1     t  t  c  t  a  g  t  g  c\n",
      "2     t  c  a  c  a  g  a  t  t\n",
      "3     g  g  a  c  a  g  g  g  c\n",
      "4     t  t  t  c  a  g  a  t  c\n",
      "...  .. .. .. .. .. .. .. .. ..\n",
      "2376  c  t  a  a  a  g  a  a  a\n",
      "2377  c  c  t  c  a  g  g  a  g\n",
      "2378  c  c  c  c  a  g  g  a  g\n",
      "2379  c  t  g  c  a  g  t  c  c\n",
      "2380  c  a  t  t  a  g  a  a  g\n",
      "\n",
      "[2381 rows x 9 columns]\n",
      "     -3 -2 -1  0  1  2  3  4  5\n",
      "0     c  a  g  g  t  t  g  g  t\n",
      "1     a  g  g  g  t  g  a  g  t\n",
      "2     c  g  g  g  t  a  t  g  t\n",
      "3     c  g  g  g  t  g  a  g  t\n",
      "4     a  t  g  g  t  g  a  g  g\n",
      "...  .. .. .. .. .. .. .. .. ..\n",
      "2074  a  a  g  g  t  a  a  g  a\n",
      "2075  g  t  g  g  t  a  a  g  t\n",
      "2076  t  g  g  g  t  a  a  g  t\n",
      "2077  a  a  g  g  t  g  t  g  t\n",
      "2078  a  a  g  g  t  a  t  g  a\n",
      "\n",
      "[2079 rows x 9 columns]\n",
      "     -5 -4 -3 -2 -1  0  1  2  3\n",
      "0     t  c  t  c  a  g  g  c  t\n",
      "1     c  t  a  c  a  g  c  t  c\n",
      "2     c  c  c  c  a  g  g  a  t\n",
      "3     c  c  c  c  a  g  g  c  t\n",
      "4     g  g  c  c  a  g  g  c  t\n",
      "...  .. .. .. .. .. .. .. .. ..\n",
      "2074  t  c  t  c  a  g  t  c  t\n",
      "2075  t  t  t  c  a  g  a  c  t\n",
      "2076  t  c  a  t  a  g  g  g  a\n",
      "2077  c  t  a  a  a  g  c  g  g\n",
      "2078  t  t  t  t  a  g  a  g  a\n",
      "\n",
      "[2079 rows x 9 columns]\n",
      "save Train_donor_signal_str successful!\n",
      "save Train_acceptor_signal_str successful!\n",
      "save Test_donor_signal_str successful!\n",
      "save Test_acceptor_signal_str successful!\n"
     ]
    }
   ],
   "source": [
    "def signal_to_csv(donor_signal_str,mode,dir=\"Train\"):\n",
    "    donor_signal_list=[list(line) for line in donor_signal_str]\n",
    "    if mode==\"donor\" or mode==\"nonDonor\":\n",
    "        col_name = list(range(-left,right))\n",
    "    elif mode==\"acceptor\":\n",
    "        col_name = list(range(-right+1,left+1))\n",
    "    donorDf = pd.DataFrame(columns=col_name, data=donor_signal_list, index=None)\n",
    "    print(donorDf)  # [2843 rows x 9 columns]\n",
    "    donorDf.to_csv(f'output/{dir}_{mode}_signal_info.csv',index=None)\n",
    "def save_donors_str(signal_str,filename=\"signal_str\"):\n",
    "    f = open(f'output/{filename}.txt', 'w')\n",
    "    for donor in signal_str:\n",
    "        donor = donor.lower()  # å­˜å…¥æ–‡ä»¶æ—¶ï¼ŒæŠŠæ‰€æœ‰å­—ç¬¦éƒ½è§„èŒƒæˆå°å†™\n",
    "        f.write(donor + '\\n')\n",
    "    f.close()\n",
    "    print(f'save {filename} successful!')\n",
    "signal_to_csv(train_donor_signal_all_str,dir=\"Train\",mode=\"donor\")\n",
    "signal_to_csv(train_acceptor_signal_all_str,dir=\"Train\",mode=\"acceptor\")\n",
    "signal_to_csv(test_donor_signal_all_str,dir=\"Test\",mode=\"donor\")\n",
    "signal_to_csv(test_acceptor_signal_all_str,dir=\"Test\",mode=\"acceptor\")\n",
    "save_donors_str(train_donor_signal_all_str,filename=\"Train_donor_signal_str\")\n",
    "save_donors_str(train_acceptor_signal_all_str,filename=\"Train_acceptor_signal_str\")\n",
    "save_donors_str(test_donor_signal_all_str,filename=\"Test_donor_signal_str\")\n",
    "save_donors_str(test_acceptor_signal_all_str,filename=\"Test_acceptor_signal_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Train_set_info(non-seq).csv !\n",
      "Created Test_set_info(non-seq).csv !\n",
      "Created Train_set_info.csv !\n",
      "Created Test_set_info.csv !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_seq_to_csv(df_set_info,seq_list,dir='Training'):\n",
    "    # è¿™é‡Œå¯ä»¥åŸºäºä¸Šä¸€ä¸ªæ·»åŠ \n",
    "    df_set_info[\"Seq\"] = seq_list\n",
    "    df_set_info.to_csv(f'output/{dir}_set_info.csv',index=None)\n",
    "    print(f\"Created {dir}_set_info.csv !\")\n",
    "#     return df_set_info\n",
    "def file_info_simple(file_path,file_locus,donor_positions,acceptor_positions,donor_signals,acceptor_signals,length_list,file_exons,dir='Training'):\n",
    "    \n",
    "    df_set_info = pd.DataFrame({'Path':file_path, 'Locus':file_locus,\"Length\":length_list,\"Exon Num\":[ len(exons)for exons  in file_exons],\"Exon Location\":file_exons,\"Donor Site\":donor_positions,\\\n",
    "                              \"Acceptor Site\":acceptor_positions,\"Donor signals\":donor_signals,\"Acceptor signals\":acceptor_signals})\n",
    "    df_set_info.to_csv(f'output/{dir}_set_info(non-seq).csv',index=None)\n",
    "    print(f\"Created {dir}_set_info(non-seq).csv !\")\n",
    "    return df_set_info\n",
    "\n",
    "train_set_info=file_info_simple(train_file_path,train_file_locus,train_file_donor_positions,\\\n",
    "                 train_file_acceptor_positions,train_file_donor_signals,train_file_acceptor_signals,\\\n",
    "                 train_file_length_list,train_file_exons,dir='Train')\n",
    "test_set_info=file_info_simple(test_file_path,test_file_locus,test_file_donor_positions,\\\n",
    "                 test_file_acceptor_positions,test_file_donor_signals,test_file_acceptor_signals,\n",
    "                 test_file_length_list,test_file_exons,dir='Test')\n",
    "add_seq_to_csv(train_set_info,train_file_seq_list,dir='Train')\n",
    "add_seq_to_csv(test_set_info,test_file_seq_list,dir='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Prior Probability:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2381/2381 [00:00<00:00, 36281.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        p(A)      p(C)      p(G)      p(T)\n",
      "-3  0.328433  0.363713  0.188576  0.119278\n",
      "-2  0.585468  0.137757  0.133557  0.143217\n",
      "-1  0.094078  0.034019  0.788744  0.083158\n",
      " 0  0.000000  0.000000  1.000000  0.000000\n",
      " 1  0.000000  0.000000  0.000000  1.000000\n",
      " 2  0.490130  0.027299  0.458211  0.024360\n",
      " 3  0.713566  0.077278  0.117598  0.091558\n",
      " 4  0.065099  0.049559  0.838303  0.047039\n",
      " 5  0.151197  0.162957  0.216716  0.469131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cal_priorProbability(donor_signal) :\n",
    "    priors = []\n",
    "    count = []\n",
    "    count= np.zeros((9,4),dtype=np.int32)\n",
    "    priors = np.zeros((9,4),dtype=np.float32)\n",
    "    # count[1][0]+1\n",
    "    donor_signal = tqdm(donor_signal, desc='Calculating Prior Probability:')\n",
    "    for donor in donor_signal:\n",
    "        # print(donor)\n",
    "        for j in range(len(donor)):\n",
    "            pos = lower_bases.index(donor[j])\n",
    "            count[j][pos] += 1\n",
    "    for i in range(len(priors)):\n",
    "        for j in range(4):\n",
    "            priors[i][j] = count[i][j]/len(donor_signal)\n",
    "    return priors\n",
    "def prior_to_csv(priors,mode):\n",
    "    pri_col_name = [f'p({base})' for base in bases ]\n",
    "    pri_row_name= list(range(-left,right))\n",
    "    \n",
    "    priorDf= pd.DataFrame(index=pri_row_name,columns=pri_col_name, data=priors)\n",
    "#     print(priorDf.isnull().sum()) #æœ‰å››åˆ—ç¼ºå¤±å€¼\n",
    "    print(priorDf)  # [2843 rows x 9 columns]\n",
    "    priorDf.to_csv(f'output/prior_probability_(P{mode}).csv')\n",
    "    \n",
    "priors_p=cal_priorProbability(train_donor_signal_all_str)\n",
    "prior_to_csv(priors_p,mode=\"+\")\n",
    "# print(priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Conditional Probability:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2381/2381 [00:00<00:00, 23352.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      p(A,A)    p(A,C)    p(A,G)    p(A,T)    p(C,A)    p(C,C)    p(C,G)  \\\n",
      "-2  0.616368  0.097187  0.149616  0.136829  0.677829  0.123557  0.058891   \n",
      "-1  0.073888  0.018651  0.854376  0.053085  0.189024  0.070122  0.524390   \n",
      " 0  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  1.000000   \n",
      " 1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      " 2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      " 3  0.646101  0.095973  0.121680  0.136247  0.676923  0.030769  0.107692   \n",
      " 4  0.057681  0.027075  0.882284  0.032961  0.163043  0.222826  0.429348   \n",
      " 5  0.212903  0.135484  0.445161  0.206452  0.296610  0.288136  0.118644   \n",
      "\n",
      "      p(C,T)    p(G,A)    p(G,C)    p(G,G)    p(G,T)    p(T,A)    p(T,C)  \\\n",
      "-2  0.139723  0.608018  0.167038  0.124722  0.100223  0.183099  0.246479   \n",
      "-1  0.216463  0.147799  0.066038  0.707547  0.078616  0.035191  0.032258   \n",
      " 0  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000   \n",
      " 1  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000   \n",
      " 2  0.000000  0.000000  0.000000  0.000000  0.000000  0.490130  0.027299   \n",
      " 3  0.184615  0.807516  0.060495  0.100825  0.031164  0.344828  0.068966   \n",
      " 4  0.184783  0.060714  0.050000  0.860714  0.028571  0.045872  0.077982   \n",
      " 5  0.296610  0.138778  0.157315  0.193888  0.510020  0.133929  0.169643   \n",
      "\n",
      "      p(T,G)    p(T,T)  \n",
      "-2  0.330986  0.239437  \n",
      "-1  0.850440  0.082111  \n",
      " 0  1.000000  0.000000  \n",
      " 1  0.000000  0.000000  \n",
      " 2  0.458211  0.024360  \n",
      " 3  0.362069  0.224138  \n",
      " 4  0.811927  0.064220  \n",
      " 5  0.410714  0.285714  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#è®¡ç®—æ¡ä»¶æ¦‚ç‡\n",
    "def cal_conditionalProbability(donor_signal,priors):\n",
    "    N,M=8,16\n",
    "    count= np.zeros((N,M),dtype=np.int32)\n",
    "    joint = np.zeros((N,M),dtype=np.float32)\n",
    "    conditional = np.zeros((N,M),dtype=np.float32)\n",
    "    # count\n",
    "    donor_signal = tqdm(donor_signal, desc='Calculating Conditional Probability:')\n",
    "    for donor in donor_signal:\n",
    "        for j in range(N):\n",
    "            base_list = [i+j for i in lower_bases for j in lower_bases ]\n",
    "            pos=base_list.index(donor[j:j+2])\n",
    "            count[j][pos] +=1\n",
    "    #  joint\n",
    "    joint= count/(len(donor_signal))\n",
    "    ## conditional P(i+1,A|i,C) = P(A,C)/P(C) ç¬¬iä½æ˜¯Cï¼Œç¬¬i+1ä½ä¸ºAçš„æ¦‚ç‡ï¼Œ\n",
    "    for i in range(8):\n",
    "        for j in range(4):\n",
    "            for m in range(4):\n",
    "                if priors[i][j]:\n",
    "                    conditional[i][4*j+m] = joint[i][4*j+m]/priors[i][j]\n",
    "                else:\n",
    "                    conditional[i][4*j+m] =0\n",
    "    return conditional\n",
    "\n",
    "def conditional_to_csv(conditional,mode):\n",
    "    conditional_col_name = [f'p({i},{j})' for i in bases for j in bases ]\n",
    "    conditional_row_name=list(range(-left+1,right))\n",
    "    conditionalDf= pd.DataFrame(index=conditional_row_name,columns=conditional_col_name, data=conditional)\n",
    "    # donorDf.isnull().sum() æœ‰å››åˆ—ç¼ºå¤±å€¼\n",
    "    print(conditionalDf)  # [2843 rows x 9 columns]\n",
    "    conditionalDf.to_csv(f'output/conditionalDf_probability(P{mode}).csv')\n",
    "    \n",
    "conditional = cal_conditionalProbability(train_donor_signal_all_str,priors_p)\n",
    "conditional_to_csv(conditional,mode=\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Non Donor Signal Sequence:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 462/462 [00:05<00:00, 88.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Non Donor Signal Sequence successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5511802"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create \n",
    "\n",
    "\n",
    "# def create_nonDonor_rand(seqs_DNA, donor_locations, acceptor_locations,randnum=400):\n",
    "#     '''\n",
    "#     # ç›®å‰æ˜¯æš´åŠ›æŠŠæ‰€æœ‰ésignalä½ç‚¹éƒ½å¯¼å‡ºï¼Œ5510888 ç®—äº†ï¼Œå¤ªå¤šäº†ï¼Œä½†æ˜¯åˆå’ŒPPTä¸Šçš„æ•°å€¼æ¯”è¾ƒç›¸è¿‘ï¼Œæˆ‘è¿˜æ˜¯é€‰å®ƒå§\n",
    "#     å¯èƒ½ä¼šå°è¯•å–éšæœºæ•°ï¼Œäº§ç”Ÿposéšæœºæ•°ï¼Œç„¶åå¯¼å‡ºç›¸åº”æ•°é‡çš„å‡ä½ç‚¹, 184800ï¼Œä½†æ˜¯è¿™æ ·å„ç¢±åŸºèƒŒæ™¯æ¦‚ç‡ä¼šåå‘ç›¸åŒ\n",
    "#     '''\n",
    "#     nonDonors = []\n",
    "#     file_num = tqdm(range(len(donor_locations)), desc='Creating Non Donor Signal Sequence:')\n",
    "#     nonDonor_temp = []\n",
    "#     for i in file_num:\n",
    "#         # æ¯ä¸ªæ–‡ä»¶å¾ªç¯\n",
    "#         file_seq_DNA = seqs_DNA[i]\n",
    "#         num = len(donor_locations[i])  # å¯»æ‰¾ç›¸åº”æ•°é‡çš„å‡ä½ç‚¹ï¼Œæˆ‘æœ‰ä¸‰ä¸ªå°±æŠ½ä¸‰ä¸ªå—\n",
    "#         length = len(file_seq_DNA)\n",
    "#         donor_signals_start=[int(pos)-1-left for pos in donor_locations[i]]\n",
    "# #         print(donor_locations[i])\n",
    "#         acceptor_signals_start=[int(pos)-right for pos in acceptor_locations[i] if int(pos)-right>=0 ]\n",
    "#         signals_start=sorted(donor_signals_start+acceptor_signals_start)\n",
    "# #         print(signals_start)\n",
    "#         count = 0\n",
    "#         pos_list=[]\n",
    "#         while count < randnum:\n",
    "# #             print(count)\n",
    "#             pos = random.randint(0,length-signal_num) \n",
    "#             if (pos not in signals_start) and (pos not in pos_list):\n",
    "#                 nonDonor = file_seq_DNA[pos:pos + signal_num]\n",
    "#                 for i in noknown_bases:\n",
    "#                     if i in nonDonor:\n",
    "#                         break\n",
    "#                 else:\n",
    "#                     nonDonor_temp.append(nonDonor)\n",
    "#                     pos_list.append(pos)\n",
    "#                     count +=1\n",
    "#         # æŠŠæ‰€æœ‰å‡ä½ç‚¹ç”Ÿæˆä¹‹åï¼Œè¿˜ç»§ç»­ç”Ÿæˆï¼Ÿ\n",
    "# #         slice = random.sample(phonyDonor_temp, length)\n",
    "# #         nonDonors.extend(slice)\n",
    "#     print('Created Non Donor Signal Sequence successful!')\n",
    "#     return nonDonor_temp\n",
    "# ç”Ÿæˆå‡donorç‰‡æ®µ\n",
    "\n",
    "def create_nonDonor(seqs_DNA, donor_locations, acceptor_locations,randnum=10):\n",
    "    '''\n",
    "    # ç›®å‰æ˜¯æš´åŠ›æŠŠæ‰€æœ‰ésignalä½ç‚¹éƒ½å¯¼å‡ºï¼Œ5510888 ç®—äº†ï¼Œå¤ªå¤šäº†\n",
    "    å¯èƒ½ä¼šå°è¯•å–éšæœºæ•°ï¼Œäº§ç”Ÿposéšæœºæ•°ï¼Œç„¶åå¯¼å‡ºç›¸åº”æ•°é‡çš„å‡ä½ç‚¹\n",
    "    '''\n",
    "    nonDonors = []\n",
    "    file_num = tqdm(range(len(donor_locations)), desc='Creating Non Donor Signal Sequence:')\n",
    "    nonDonor_temp = []\n",
    "    for i in file_num:\n",
    "        # æ¯ä¸ªæ–‡ä»¶å¾ªç¯\n",
    "#         print(i)\n",
    "#         nonDonor_temp = []\n",
    "        file_seq_DNA = seqs_DNA[i]\n",
    "        num = len(donor_locations[i])  # å¯»æ‰¾ç›¸åº”æ•°é‡çš„å‡ä½ç‚¹ï¼Œæˆ‘æœ‰ä¸‰ä¸ªå°±æŠ½ä¸‰ä¸ªå—\n",
    "        length = len(file_seq_DNA)\n",
    "        donor_signals_start=[int(pos)-1-left for pos in donor_locations[i]]\n",
    "#         print(donor_locations[i])\n",
    "        acceptor_signals_start=[int(pos)-right for pos in acceptor_locations[i] if int(pos)-right>=0 ]\n",
    "        signals_start=sorted(donor_signals_start+acceptor_signals_start)\n",
    "#         print(signals_start) \n",
    "        for index in range(length-signal_num+1):\n",
    "            if index not in signals_start:\n",
    "                nonDonor = file_seq_DNA[index:index + signal_num]\n",
    "                for i in noknown_bases:\n",
    "                    if i in nonDonor:\n",
    "                        break\n",
    "                else:\n",
    "                    nonDonor_temp.append(nonDonor)\n",
    "        # æŠŠæ‰€æœ‰å‡ä½ç‚¹ç”Ÿæˆä¹‹åï¼Œè¿˜ç»§ç»­ç”Ÿæˆï¼Ÿ\n",
    "#         slice = random.sample(phonyDonor_temp, length)\n",
    "#         nonDonors.extend(slice)\n",
    "    print('Created Non Donor Signal Sequence successful!')\n",
    "    return nonDonor_temp\n",
    "\n",
    "nonDonor_list=create_nonDonor(train_file_seq_list, train_file_donor_positions, train_file_acceptor_positions)\n",
    "len(nonDonor_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ°å¯¼å‡ºäº†4592ä¸ªédonor site siganl åºåˆ—ï¼ˆå› ä¸ºå‰”é™¤äº†å¾ˆå¤šæœªçŸ¥ç¢±åŸºå§?æŒ‰é“ç†æ¥è¯´è¿™ç§æ–¹å¼ä¸åº”è¯¥å‡ºç°è¶…ä¸‡ä¸ªçš„åºåˆ—å—ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ)  \n",
    "å¥½å§ï¼Œæˆ‘çŸ¥é“äº†ï¼ŒåŸæ¥åˆæ˜¯å¾ªç¯ä½ç½®æ”¾é”™  \n",
    "ç°åœ¨å¯¼å‡ºäº†5511802ä¸ª  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting  Base Distribution:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5511802/5511802 [00:07<00:00, 694850.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 11520141, 't': 13457006, 'a': 12904584, 'g': 11724487}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_each_char(nonDonor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        -3 -2 -1  0  1  2  3  4  5\n",
      "0        c  t  t  t  a  a  t  t  t\n",
      "1        t  t  t  a  a  t  t  t  t\n",
      "2        t  t  a  a  t  t  t  t  a\n",
      "3        t  a  a  t  t  t  t  a  t\n",
      "4        a  a  t  t  t  t  a  t  c\n",
      "...     .. .. .. .. .. .. .. .. ..\n",
      "5511797  c  t  t  t  a  g  a  t  g\n",
      "5511798  t  t  t  a  g  a  t  g  g\n",
      "5511799  t  t  a  g  a  t  g  g  a\n",
      "5511800  t  a  g  a  t  g  g  a  g\n",
      "5511801  a  g  a  t  g  g  a  g  a\n",
      "\n",
      "[5511802 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Prior Probability:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5511802/5511802 [02:19<00:00, 39552.68it/s]\n",
      "Calculating Conditional Probability::   0%|          | 2393/5511802 [00:00<03:50, 23929.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        p(A)      p(C)      p(G)      p(T)\n",
      "-3  0.260176  0.232064  0.236535  0.271225\n",
      "-2  0.260070  0.232180  0.236552  0.271198\n",
      "-1  0.260208  0.232249  0.236201  0.271342\n",
      " 0  0.260328  0.232092  0.236200  0.271381\n",
      " 1  0.259927  0.232399  0.236639  0.271034\n",
      " 2  0.260156  0.232376  0.236024  0.271444\n",
      " 3  0.259963  0.232284  0.236368  0.271385\n",
      " 4  0.260241  0.232256  0.236185  0.271318\n",
      " 5  0.260195  0.232187  0.236456  0.271162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Conditional Probability:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5511802/5511802 [03:50<00:00, 23919.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      p(A,A)    p(A,C)    p(A,G)    p(A,T)    p(C,A)    p(C,C)    p(C,G)  \\\n",
      "-2  0.296746  0.187205  0.281931  0.234117  0.306976  0.294569  0.075583   \n",
      "-1  0.297091  0.187342  0.281303  0.234265  0.307077  0.294593  0.075395   \n",
      " 0  0.297018  0.187027  0.281816  0.234139  0.307245  0.294354  0.075511   \n",
      " 1  0.296868  0.187233  0.281858  0.234041  0.306103  0.295006  0.075614   \n",
      " 2  0.297431  0.187513  0.280660  0.234396  0.307051  0.294625  0.075501   \n",
      " 3  0.296638  0.187263  0.281964  0.234135  0.307043  0.294606  0.075506   \n",
      " 4  0.297221  0.187353  0.281156  0.234271  0.307098  0.294596  0.075449   \n",
      " 5  0.296928  0.187163  0.281862  0.234047  0.307079  0.294574  0.075490   \n",
      "\n",
      "      p(C,T)    p(G,A)    p(G,C)    p(G,G)    p(G,T)    p(T,A)    p(T,C)  \\\n",
      "-2  0.322872  0.257471  0.230251  0.293022  0.219256  0.187022  0.223623   \n",
      "-1  0.322935  0.257576  0.230293  0.292836  0.219296  0.187008  0.223645   \n",
      " 0  0.322891  0.257991  0.230343  0.292053  0.219613  0.187020  0.223537   \n",
      " 1  0.323277  0.258019  0.230621  0.293517  0.217843  0.186661  0.223730   \n",
      " 2  0.322823  0.257559  0.230187  0.292982  0.219271  0.186465  0.223937   \n",
      " 3  0.322844  0.257141  0.230535  0.292686  0.219638  0.186962  0.223601   \n",
      " 4  0.322857  0.257617  0.230291  0.292872  0.219220  0.186997  0.223622   \n",
      " 5  0.322857  0.257730  0.230305  0.293172  0.218793  0.186974  0.223606   \n",
      "\n",
      "      p(T,G)    p(T,T)  \n",
      "-2  0.281501  0.307854  \n",
      "-1  0.281220  0.308127  \n",
      " 0  0.281375  0.308069  \n",
      " 1  0.281472  0.308138  \n",
      " 2  0.281126  0.308472  \n",
      " 3  0.281411  0.308026  \n",
      " 4  0.281312  0.308069  \n",
      " 5  0.281325  0.308095  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "signal_to_csv(nonDonor_list,mode=\"nonDonor\") # 5510888\n",
    "priors_n=cal_priorProbability(nonDonor_list)\n",
    "prior_to_csv(priors_n,mode=\"-\")\n",
    "conditional_n = cal_conditionalProbability(nonDonor_list,priors_n)\n",
    "conditional_to_csv(conditional_n,mode=\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ•°æ®å¤„ç†éƒ¨åˆ†ç»“æŸï¼ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
