{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-da8d16df01c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# !pip install watermark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ! pip install seaborn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwatermark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "# !pip install pandas\n",
    "# !pip install watermark\n",
    "# ! pip install seaborn\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import watermark\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls\n",
    "# !unzip 'Training and testing datasets.zip'  -d './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2021-04-30T09:05:39.938453+08:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.7.3\n",
      "IPython version      : 7.5.0\n",
      "\n",
      "Compiler    : GCC 7.3.0\n",
      "OS          : Linux\n",
      "Release     : 4.18.0-80.7.1.el8_0.x86_64\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 52\n",
      "Architecture: 64bit\n",
      "\n",
      "pandas   : 1.2.4\n",
      "numpy    : 1.20.2\n",
      "re       : 2.2.1\n",
      "watermark: 2.2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "- 包装脚本为类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDATA CLEAN\\ntrain set contains 462 files, all suffies of file is \"TXT\",the letters are  lowercase  letters.\\ntest set contains 570 files, the suffixes of file contains \"TXT\",\"txt\",and the letters are capital letters.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "DATA CLEAN\n",
    "train set contains 462 files, all suffies of file is \"TXT\",the letters are  lowercase  letters.\n",
    "test set contains 570 files, the suffixes of file contains \"TXT\",\"txt\",and the letters are capital letters.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "left=3\n",
    "right=6 \n",
    "signal_num = left+right\n",
    "bases=\"ACGT\"\n",
    "lower_bases=\"acgt\"\n",
    "noknown_bases=\"nksrywmbv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b3a53ed88764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-b3a53ed88764>\u001b[0m in \u001b[0;36mmkdir\u001b[0;34m(dirs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def mkdir(dirs):\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    else:\n",
    "        pass\n",
    "mkdir(\"hh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOADING Training Set: 100%|██████████| 462/462 [00:00<00:00, 359311.78it/s]\n",
      "LOADING Testing Set: 100%|██████████| 570/570 [00:00<00:00, 404389.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462 Files Loading Finished!\n",
      "\n",
      "570 Files Loading Finished!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##### 读取Training set文件夹下的所有文件 #####\n",
    "def loadFile(file_dir):\n",
    "    '''\n",
    "    Test 文件夹的后缀名有txt也有TXT,另外我希望这个函数不加载非txt文件\n",
    "    parameter：file_dir\n",
    "    output:file_path,locus_list\n",
    "    '''\n",
    "    file_path = []\n",
    "    file_locus_list = []\n",
    "    count=0\n",
    "    all_file = tqdm(os.listdir(file_dir), desc=f'LOADING {file_dir}')\n",
    "    for file_name in all_file:\n",
    "        count+=1\n",
    "        suffix = file_name.split(\".\")[1].lower()\n",
    "        # 用正则提取suffix = re.findall(\"\\.(.+$)\",file_name)[-1].lower()\n",
    "        if suffix != \"txt\":\n",
    "            continue\n",
    "        file_locus_list.append(re.search(\"(.+?)\\.\",file_name).group(1))\n",
    "        path = f\"{file_dir}/{file_name}\"\n",
    "        file_path.append(path)\n",
    "    all_file.write(f\"{count} Files Loading Finished!\")\n",
    "    all_file.close()\n",
    "    print()\n",
    "    return file_path,file_locus_list\n",
    "train_file_path,train_file_locus =loadFile('Training Set')\n",
    "test_file_path,test_file_locus =loadFile('Testing Set')\n",
    "# print('LOADING File'.center(50,'*'))\n",
    "# print(train_file_path)\n",
    "# print(len(train_file_locus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Progressing：:  32%|███▏      | 148/462 [00:00<00:00, 1132.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Extract Train Set donor signals**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Progressing：: 100%|██████████| 462/462 [00:00<00:00, 1396.89it/s]\n",
      "Test Progressing：:  33%|███▎      | 188/570 [00:00<00:00, 1874.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n",
      "**********Extract Test Set donor signals**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Progressing：: 100%|██████████| 570/570 [00:00<00:00, 1812.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##### 读取每个文件，抽取donor site 前3和后6共九个碱基 #####\n",
    "def extract_donor_signal(file_path,dir='Train'):\n",
    "    '''\n",
    "    parameter：train_file_path|test_file_path\n",
    "    output:file_donor_positions,file_acceptor_positions,file_donor_signals,donor_signal_all\n",
    "    '''\n",
    "    print(f'Extract {dir} Set donor signals'.center(50, '*'))\n",
    "    file_donor_positions= []\n",
    "    file_acceptor_positions= []\n",
    "    file_donor_signals=[]\n",
    "    file_acceptor_signals=[]\n",
    "    all_donor_signal_str=[]\n",
    "    all_acceptor_signal_str=[]\n",
    "    file_length_list = []\n",
    "    file_seq_list = []\n",
    "    file_exons = []\n",
    "    # positions=[]\n",
    "    # print(\"Progressing：\",end='')\n",
    "    file_path = tqdm(file_path, desc=f'{dir} Progressing：')\n",
    "    for file in file_path:\n",
    "        f = open(file)\n",
    "        _=f.readline() # pass the first line \n",
    "        second_line=f.readline()  \n",
    "        # extract  donor and acceptor site\n",
    "        donor_positions_list = re.findall(r'\\d+\\.\\.(\\d+)',second_line)[:-1] \n",
    "        donor_positions_list = [int(pos)+1 for pos in donor_positions_list] \n",
    "        acceptor_positions_list = re.findall(r'(\\d+)\\.\\.\\d+',second_line)[1:]\n",
    "        acceptor_positions_list = [int(pos)-1 for pos in acceptor_positions_list]\n",
    "        exon_positions_list = re.findall(r'(\\d+)\\.\\.(\\d+)',second_line)\n",
    "        file_exons.append(exon_positions_list)\n",
    "        file_donor_positions.append(donor_positions_list)\n",
    "        file_acceptor_positions.append(acceptor_positions_list)\n",
    "        seq = ''\n",
    "        for line in f.readlines():\n",
    "                seq += line.strip()\n",
    "        # extract  donor site signal\n",
    "        seq_length = len(seq)\n",
    "        file_seq_list.append(seq)\n",
    "        file_length_list.append(seq_length)\n",
    "        donor_signal=[]\n",
    "        for pos in donor_positions_list:\n",
    "            # print(pos)\n",
    "            if pos+right-1<=seq_length:\n",
    "                signal_range = seq[pos-1-left:pos-1+right].lower() # Lowercase letters\n",
    "            else:\n",
    "                continue\n",
    "            # seq_range = seq[pos-left:pos+right] # 有4个位点就是正好在序列最后， 所以需要剔除\n",
    "            # donor_signal.append(list(seq_range))\n",
    "            donor_signal.append(signal_range)\n",
    "            all_donor_signal_str.append(signal_range)\n",
    "        file_donor_signals.append(donor_signal)\n",
    "        acceptor_signal=[]\n",
    "        for pos in acceptor_positions_list:\n",
    "            # print(pos)\n",
    "            if pos-right>=0:\n",
    "                signal_range = seq[pos-right:pos+left].lower() # Lowercase letters\n",
    "            else:\n",
    "                continue \n",
    "            acceptor_signal.append(signal_range)\n",
    "            all_acceptor_signal_str.append(signal_range)\n",
    "        file_acceptor_signals.append(acceptor_signal)\n",
    "    print(\"Finished!\")\n",
    "    return file_donor_positions,file_donor_signals,all_donor_signal_str,file_acceptor_positions,\\\n",
    "file_acceptor_signals,all_acceptor_signal_str,file_length_list,file_seq_list,file_exons\n",
    "\n",
    "#  str= \"CDSjoin(28199..28271,28881..28988,34291..34586)\"\n",
    "#  exon_positions_list = re.findall(r'(\\d+)\\.\\.(\\d+)',str)\n",
    "#  print(exon_positions_list)\n",
    "\n",
    "train_file_donor_positions,train_file_donor_signals,train_donor_signal_all_str,\\\n",
    "train_file_acceptor_positions,train_file_acceptor_signals,train_acceptor_signal_all_str,\\\n",
    "train_file_length_list,train_file_seq_list,train_file_exons=extract_donor_signal(train_file_path,dir=\"Train\")\n",
    "\n",
    "test_file_donor_positions,test_file_donor_signals,test_donor_signal_all_str,\\\n",
    "test_file_acceptor_positions,test_file_acceptor_signals,test_acceptor_signal_all_str,\\\n",
    "test_file_length_list,test_file_seq_list,test_file_exons=extract_donor_signal(test_file_path,dir=\"Test\")\n",
    "# print(train_file_donor_positions)\n",
    "# print(train_file_acceptor_positions)\n",
    "# print(train_file_donor_signals[0])\n",
    "# print(len(train_file_donor_signals)) # return the num of rows\n",
    "# print(train_donor_signal_all_str)\n",
    "# print(test_file_donor_positions)\n",
    "# print(test_file_acceptor_positions)\n",
    "# print(test_file_donor_signals[0])\n",
    "# print(len(test_file_donor_signals)) # return the num of rows\n",
    "# print(test_donor_signal_all_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting  Base Distribution:: 100%|██████████| 462/462 [00:00<00:00, 656.83it/s]\n",
      "Counting  Base Distribution:: 100%|██████████| 2381/2381 [00:00<00:00, 652764.09it/s]\n",
      "Counting  Base Distribution:: 100%|██████████| 570/570 [00:00<00:00, 1583.46it/s]\n",
      "Counting  Base Distribution:: 100%|██████████| 2079/2079 [00:00<00:00, 646401.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of all bases in training set\n",
      " {'c': 1282733, 't': 1498203, 'a': 1436966, 'g': 1306260, 'n': 682, 'k': 28, 'b': 4, 's': 27, 'm': 16, 'r': 15, 'v': 2, 'y': 26, 'w': 14}\n",
      "Distribution of donor site bases in training  set\n",
      " {'c': 2030, 'g': 8909, 't': 4709, 'a': 5781}\n",
      "Distribution of all bases in testing set\n",
      " {'G': 702343, 'A': 736600, 'T': 762900, 'C': 689433, 'N': 862, 'Y': 3, 'R': 5, 'B': 1, 'S': 1, 'K': 1}\n",
      "Distribution of donor site  bases in testing set\n",
      " {'c': 1691, 'a': 5114, 'g': 7730, 't': 4176}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 才发现文本里不只是有ACGT，还有其他数值\n",
    "def count_each_char(str_list):\n",
    "    dict = {}\n",
    "    str_list = tqdm(str_list, desc='Counting  Base Distribution:')\n",
    "    for seq in str_list:\n",
    "         for i  in seq:\n",
    "            if i not in dict:\n",
    "                dict[i] = 1\n",
    "            else:\n",
    "                dict[i] += 1\n",
    "    return dict\n",
    "\n",
    "base_dis_1 = count_each_char(train_file_seq_list)\n",
    "base_dis_2 = count_each_char(train_donor_signal_all_str)\n",
    "base_dis_3 = count_each_char(test_file_seq_list)\n",
    "base_dis_4 = count_each_char(test_donor_signal_all_str)\n",
    "print(\"Distribution of all bases in training set\\n\",base_dis_1)\n",
    "print(\"Distribution of donor site bases in training  set\\n\",base_dis_2)\n",
    "print(\"Distribution of all bases in testing set\\n\",base_dis_3)\n",
    "print(\"Distribution of donor site  bases in testing set\\n\",base_dis_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -3 -2 -1  0  1  2  3  4  5\n",
      "0     c  g  g  g  t  a  t  g  t\n",
      "1     g  c  a  g  t  g  a  g  t\n",
      "2     a  a  t  g  t  a  a  g  t\n",
      "3     c  a  g  g  t  a  a  g  a\n",
      "4     a  a  g  g  t  g  a  g  t\n",
      "...  .. .. .. .. .. .. .. .. ..\n",
      "2376  a  a  g  g  t  a  a  g  a\n",
      "2377  c  a  g  g  t  g  a  g  a\n",
      "2378  a  a  g  g  t  a  a  g  g\n",
      "2379  c  a  g  g  t  g  a  g  g\n",
      "2380  t  c  g  g  t  g  a  g  t\n",
      "\n",
      "[2381 rows x 9 columns]\n",
      "     -5 -4 -3 -2 -1  0  1  2  3\n",
      "0     c  c  c  c  a  g  g  a  t\n",
      "1     t  t  c  t  a  g  t  g  c\n",
      "2     t  c  a  c  a  g  a  t  t\n",
      "3     g  g  a  c  a  g  g  g  c\n",
      "4     t  t  t  c  a  g  a  t  c\n",
      "...  .. .. .. .. .. .. .. .. ..\n",
      "2376  c  t  a  a  a  g  a  a  a\n",
      "2377  c  c  t  c  a  g  g  a  g\n",
      "2378  c  c  c  c  a  g  g  a  g\n",
      "2379  c  t  g  c  a  g  t  c  c\n",
      "2380  c  a  t  t  a  g  a  a  g\n",
      "\n",
      "[2381 rows x 9 columns]\n",
      "     -3 -2 -1  0  1  2  3  4  5\n",
      "0     c  a  g  g  t  t  g  g  t\n",
      "1     a  g  g  g  t  g  a  g  t\n",
      "2     c  g  g  g  t  a  t  g  t\n",
      "3     c  g  g  g  t  g  a  g  t\n",
      "4     a  t  g  g  t  g  a  g  g\n",
      "...  .. .. .. .. .. .. .. .. ..\n",
      "2074  a  a  g  g  t  a  a  g  a\n",
      "2075  g  t  g  g  t  a  a  g  t\n",
      "2076  t  g  g  g  t  a  a  g  t\n",
      "2077  a  a  g  g  t  g  t  g  t\n",
      "2078  a  a  g  g  t  a  t  g  a\n",
      "\n",
      "[2079 rows x 9 columns]\n",
      "     -5 -4 -3 -2 -1  0  1  2  3\n",
      "0     t  c  t  c  a  g  g  c  t\n",
      "1     c  t  a  c  a  g  c  t  c\n",
      "2     c  c  c  c  a  g  g  a  t\n",
      "3     c  c  c  c  a  g  g  c  t\n",
      "4     g  g  c  c  a  g  g  c  t\n",
      "...  .. .. .. .. .. .. .. .. ..\n",
      "2074  t  c  t  c  a  g  t  c  t\n",
      "2075  t  t  t  c  a  g  a  c  t\n",
      "2076  t  c  a  t  a  g  g  g  a\n",
      "2077  c  t  a  a  a  g  c  g  g\n",
      "2078  t  t  t  t  a  g  a  g  a\n",
      "\n",
      "[2079 rows x 9 columns]\n",
      "save Train_donor_signal_str successful!\n",
      "save Train_acceptor_signal_str successful!\n",
      "save Test_donor_signal_str successful!\n",
      "save Test_acceptor_signal_str successful!\n"
     ]
    }
   ],
   "source": [
    "def signal_to_csv(donor_signal_str,mode,dir=\"Train\"):\n",
    "    donor_signal_list=[list(line) for line in donor_signal_str]\n",
    "    if mode==\"donor\" or mode==\"nonDonor\":\n",
    "        col_name = list(range(-left,right))\n",
    "    elif mode==\"acceptor\":\n",
    "        col_name = list(range(-right+1,left+1))\n",
    "    donorDf = pd.DataFrame(columns=col_name, data=donor_signal_list, index=None)\n",
    "    print(donorDf)  # [2843 rows x 9 columns]\n",
    "    donorDf.to_csv(f'output/{dir}_{mode}_signal_info.csv',index=None)\n",
    "def save_donors_str(signal_str,filename=\"signal_str\"):\n",
    "    f = open(f'output/{filename}.txt', 'w')\n",
    "    for donor in signal_str:\n",
    "        donor = donor.lower()  # 存入文件时，把所有字符都规范成小写\n",
    "        f.write(donor + '\\n')\n",
    "    f.close()\n",
    "    print(f'save {filename} successful!')\n",
    "signal_to_csv(train_donor_signal_all_str,dir=\"Train\",mode=\"donor\")\n",
    "signal_to_csv(train_acceptor_signal_all_str,dir=\"Train\",mode=\"acceptor\")\n",
    "signal_to_csv(test_donor_signal_all_str,dir=\"Test\",mode=\"donor\")\n",
    "signal_to_csv(test_acceptor_signal_all_str,dir=\"Test\",mode=\"acceptor\")\n",
    "save_donors_str(train_donor_signal_all_str,filename=\"Train_donor_signal_str\")\n",
    "save_donors_str(train_acceptor_signal_all_str,filename=\"Train_acceptor_signal_str\")\n",
    "save_donors_str(test_donor_signal_all_str,filename=\"Test_donor_signal_str\")\n",
    "save_donors_str(test_acceptor_signal_all_str,filename=\"Test_acceptor_signal_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Train_set_info(non-seq).csv !\n",
      "Created Test_set_info(non-seq).csv !\n",
      "Created Train_set_info.csv !\n",
      "Created Test_set_info.csv !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_seq_to_csv(df_set_info,seq_list,dir='Training'):\n",
    "    # 这里可以基于上一个添加\n",
    "    df_set_info[\"Seq\"] = seq_list\n",
    "    df_set_info.to_csv(f'output/{dir}_set_info.csv',index=None)\n",
    "    print(f\"Created {dir}_set_info.csv !\")\n",
    "#     return df_set_info\n",
    "def file_info_simple(file_path,file_locus,donor_positions,acceptor_positions,donor_signals,acceptor_signals,length_list,file_exons,dir='Training'):\n",
    "    \n",
    "    df_set_info = pd.DataFrame({'Path':file_path, 'Locus':file_locus,\"Length\":length_list,\"Exon Num\":[ len(exons)for exons  in file_exons],\"Exon Location\":file_exons,\"Donor Site\":donor_positions,\\\n",
    "                              \"Acceptor Site\":acceptor_positions,\"Donor signals\":donor_signals,\"Acceptor signals\":acceptor_signals})\n",
    "    df_set_info.to_csv(f'output/{dir}_set_info(non-seq).csv',index=None)\n",
    "    print(f\"Created {dir}_set_info(non-seq).csv !\")\n",
    "    return df_set_info\n",
    "\n",
    "train_set_info=file_info_simple(train_file_path,train_file_locus,train_file_donor_positions,\\\n",
    "                 train_file_acceptor_positions,train_file_donor_signals,train_file_acceptor_signals,\\\n",
    "                 train_file_length_list,train_file_exons,dir='Train')\n",
    "test_set_info=file_info_simple(test_file_path,test_file_locus,test_file_donor_positions,\\\n",
    "                 test_file_acceptor_positions,test_file_donor_signals,test_file_acceptor_signals,\n",
    "                 test_file_length_list,test_file_exons,dir='Test')\n",
    "add_seq_to_csv(train_set_info,train_file_seq_list,dir='Train')\n",
    "add_seq_to_csv(test_set_info,test_file_seq_list,dir='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Prior Probability:: 100%|██████████| 2381/2381 [00:00<00:00, 36281.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        p(A)      p(C)      p(G)      p(T)\n",
      "-3  0.328433  0.363713  0.188576  0.119278\n",
      "-2  0.585468  0.137757  0.133557  0.143217\n",
      "-1  0.094078  0.034019  0.788744  0.083158\n",
      " 0  0.000000  0.000000  1.000000  0.000000\n",
      " 1  0.000000  0.000000  0.000000  1.000000\n",
      " 2  0.490130  0.027299  0.458211  0.024360\n",
      " 3  0.713566  0.077278  0.117598  0.091558\n",
      " 4  0.065099  0.049559  0.838303  0.047039\n",
      " 5  0.151197  0.162957  0.216716  0.469131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cal_priorProbability(donor_signal) :\n",
    "    priors = []\n",
    "    count = []\n",
    "    count= np.zeros((9,4),dtype=np.int32)\n",
    "    priors = np.zeros((9,4),dtype=np.float32)\n",
    "    # count[1][0]+1\n",
    "    donor_signal = tqdm(donor_signal, desc='Calculating Prior Probability:')\n",
    "    for donor in donor_signal:\n",
    "        # print(donor)\n",
    "        for j in range(len(donor)):\n",
    "            pos = lower_bases.index(donor[j])\n",
    "            count[j][pos] += 1\n",
    "    for i in range(len(priors)):\n",
    "        for j in range(4):\n",
    "            priors[i][j] = count[i][j]/len(donor_signal)\n",
    "    return priors\n",
    "def prior_to_csv(priors,mode):\n",
    "    pri_col_name = [f'p({base})' for base in bases ]\n",
    "    pri_row_name= list(range(-left,right))\n",
    "    \n",
    "    priorDf= pd.DataFrame(index=pri_row_name,columns=pri_col_name, data=priors)\n",
    "#     print(priorDf.isnull().sum()) #有四列缺失值\n",
    "    print(priorDf)  # [2843 rows x 9 columns]\n",
    "    priorDf.to_csv(f'output/prior_probability_(P{mode}).csv')\n",
    "    \n",
    "priors_p=cal_priorProbability(train_donor_signal_all_str)\n",
    "prior_to_csv(priors_p,mode=\"+\")\n",
    "# print(priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Conditional Probability:: 100%|██████████| 2381/2381 [00:00<00:00, 23352.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      p(A,A)    p(A,C)    p(A,G)    p(A,T)    p(C,A)    p(C,C)    p(C,G)  \\\n",
      "-2  0.616368  0.097187  0.149616  0.136829  0.677829  0.123557  0.058891   \n",
      "-1  0.073888  0.018651  0.854376  0.053085  0.189024  0.070122  0.524390   \n",
      " 0  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  1.000000   \n",
      " 1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      " 2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      " 3  0.646101  0.095973  0.121680  0.136247  0.676923  0.030769  0.107692   \n",
      " 4  0.057681  0.027075  0.882284  0.032961  0.163043  0.222826  0.429348   \n",
      " 5  0.212903  0.135484  0.445161  0.206452  0.296610  0.288136  0.118644   \n",
      "\n",
      "      p(C,T)    p(G,A)    p(G,C)    p(G,G)    p(G,T)    p(T,A)    p(T,C)  \\\n",
      "-2  0.139723  0.608018  0.167038  0.124722  0.100223  0.183099  0.246479   \n",
      "-1  0.216463  0.147799  0.066038  0.707547  0.078616  0.035191  0.032258   \n",
      " 0  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000   \n",
      " 1  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000   \n",
      " 2  0.000000  0.000000  0.000000  0.000000  0.000000  0.490130  0.027299   \n",
      " 3  0.184615  0.807516  0.060495  0.100825  0.031164  0.344828  0.068966   \n",
      " 4  0.184783  0.060714  0.050000  0.860714  0.028571  0.045872  0.077982   \n",
      " 5  0.296610  0.138778  0.157315  0.193888  0.510020  0.133929  0.169643   \n",
      "\n",
      "      p(T,G)    p(T,T)  \n",
      "-2  0.330986  0.239437  \n",
      "-1  0.850440  0.082111  \n",
      " 0  1.000000  0.000000  \n",
      " 1  0.000000  0.000000  \n",
      " 2  0.458211  0.024360  \n",
      " 3  0.362069  0.224138  \n",
      " 4  0.811927  0.064220  \n",
      " 5  0.410714  0.285714  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#计算条件概率\n",
    "def cal_conditionalProbability(donor_signal,priors):\n",
    "    N,M=8,16\n",
    "    count= np.zeros((N,M),dtype=np.int32)\n",
    "    joint = np.zeros((N,M),dtype=np.float32)\n",
    "    conditional = np.zeros((N,M),dtype=np.float32)\n",
    "    # count\n",
    "    donor_signal = tqdm(donor_signal, desc='Calculating Conditional Probability:')\n",
    "    for donor in donor_signal:\n",
    "        for j in range(N):\n",
    "            base_list = [i+j for i in lower_bases for j in lower_bases ]\n",
    "            pos=base_list.index(donor[j:j+2])\n",
    "            count[j][pos] +=1\n",
    "    #  joint\n",
    "    joint= count/(len(donor_signal))\n",
    "    ## conditional P(i+1,A|i,C) = P(A,C)/P(C) 第i位是C，第i+1位为A的概率，\n",
    "    for i in range(8):\n",
    "        for j in range(4):\n",
    "            for m in range(4):\n",
    "                if priors[i][j]:\n",
    "                    conditional[i][4*j+m] = joint[i][4*j+m]/priors[i][j]\n",
    "                else:\n",
    "                    conditional[i][4*j+m] =0\n",
    "    return conditional\n",
    "\n",
    "def conditional_to_csv(conditional,mode):\n",
    "    conditional_col_name = [f'p({i},{j})' for i in bases for j in bases ]\n",
    "    conditional_row_name=list(range(-left+1,right))\n",
    "    conditionalDf= pd.DataFrame(index=conditional_row_name,columns=conditional_col_name, data=conditional)\n",
    "    # donorDf.isnull().sum() 有四列缺失值\n",
    "    print(conditionalDf)  # [2843 rows x 9 columns]\n",
    "    conditionalDf.to_csv(f'output/conditionalDf_probability(P{mode}).csv')\n",
    "    \n",
    "conditional = cal_conditionalProbability(train_donor_signal_all_str,priors_p)\n",
    "conditional_to_csv(conditional,mode=\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Non Donor Signal Sequence:: 100%|██████████| 462/462 [00:05<00:00, 88.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Non Donor Signal Sequence successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5511802"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create \n",
    "\n",
    "\n",
    "# def create_nonDonor_rand(seqs_DNA, donor_locations, acceptor_locations,randnum=400):\n",
    "#     '''\n",
    "#     # 目前是暴力把所有非signal位点都导出，5510888 算了，太多了，但是又和PPT上的数值比较相近，我还是选它吧\n",
    "#     可能会尝试取随机数，产生pos随机数，然后导出相应数量的假位点, 184800，但是这样各碱基背景概率会偏向相同\n",
    "#     '''\n",
    "#     nonDonors = []\n",
    "#     file_num = tqdm(range(len(donor_locations)), desc='Creating Non Donor Signal Sequence:')\n",
    "#     nonDonor_temp = []\n",
    "#     for i in file_num:\n",
    "#         # 每个文件循环\n",
    "#         file_seq_DNA = seqs_DNA[i]\n",
    "#         num = len(donor_locations[i])  # 寻找相应数量的假位点，我有三个就抽三个吗\n",
    "#         length = len(file_seq_DNA)\n",
    "#         donor_signals_start=[int(pos)-1-left for pos in donor_locations[i]]\n",
    "# #         print(donor_locations[i])\n",
    "#         acceptor_signals_start=[int(pos)-right for pos in acceptor_locations[i] if int(pos)-right>=0 ]\n",
    "#         signals_start=sorted(donor_signals_start+acceptor_signals_start)\n",
    "# #         print(signals_start)\n",
    "#         count = 0\n",
    "#         pos_list=[]\n",
    "#         while count < randnum:\n",
    "# #             print(count)\n",
    "#             pos = random.randint(0,length-signal_num) \n",
    "#             if (pos not in signals_start) and (pos not in pos_list):\n",
    "#                 nonDonor = file_seq_DNA[pos:pos + signal_num]\n",
    "#                 for i in noknown_bases:\n",
    "#                     if i in nonDonor:\n",
    "#                         break\n",
    "#                 else:\n",
    "#                     nonDonor_temp.append(nonDonor)\n",
    "#                     pos_list.append(pos)\n",
    "#                     count +=1\n",
    "#         # 把所有假位点生成之后，还继续生成？\n",
    "# #         slice = random.sample(phonyDonor_temp, length)\n",
    "# #         nonDonors.extend(slice)\n",
    "#     print('Created Non Donor Signal Sequence successful!')\n",
    "#     return nonDonor_temp\n",
    "# 生成假donor片段\n",
    "\n",
    "def create_nonDonor(seqs_DNA, donor_locations, acceptor_locations,randnum=10):\n",
    "    '''\n",
    "    # 目前是暴力把所有非signal位点都导出，5510888 算了，太多了\n",
    "    可能会尝试取随机数，产生pos随机数，然后导出相应数量的假位点\n",
    "    '''\n",
    "    nonDonors = []\n",
    "    file_num = tqdm(range(len(donor_locations)), desc='Creating Non Donor Signal Sequence:')\n",
    "    nonDonor_temp = []\n",
    "    for i in file_num:\n",
    "        # 每个文件循环\n",
    "#         print(i)\n",
    "#         nonDonor_temp = []\n",
    "        file_seq_DNA = seqs_DNA[i]\n",
    "        num = len(donor_locations[i])  # 寻找相应数量的假位点，我有三个就抽三个吗\n",
    "        length = len(file_seq_DNA)\n",
    "        donor_signals_start=[int(pos)-1-left for pos in donor_locations[i]]\n",
    "#         print(donor_locations[i])\n",
    "        acceptor_signals_start=[int(pos)-right for pos in acceptor_locations[i] if int(pos)-right>=0 ]\n",
    "        signals_start=sorted(donor_signals_start+acceptor_signals_start)\n",
    "#         print(signals_start) \n",
    "        for index in range(length-signal_num+1):\n",
    "            if index not in signals_start:\n",
    "                nonDonor = file_seq_DNA[index:index + signal_num]\n",
    "                for i in noknown_bases:\n",
    "                    if i in nonDonor:\n",
    "                        break\n",
    "                else:\n",
    "                    nonDonor_temp.append(nonDonor)\n",
    "        # 把所有假位点生成之后，还继续生成？\n",
    "#         slice = random.sample(phonyDonor_temp, length)\n",
    "#         nonDonors.extend(slice)\n",
    "    print('Created Non Donor Signal Sequence successful!')\n",
    "    return nonDonor_temp\n",
    "\n",
    "nonDonor_list=create_nonDonor(train_file_seq_list, train_file_donor_positions, train_file_acceptor_positions)\n",
    "len(nonDonor_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到导出了4592个非donor site siganl 序列（因为剔除了很多未知碱基吧?按道理来说这种方式不应该出现超万个的序列吗？？？？)  \n",
    "好吧，我知道了，原来又是循环位置放错  \n",
    "现在导出了5511802个  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting  Base Distribution:: 100%|██████████| 5511802/5511802 [00:07<00:00, 694850.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 11520141, 't': 13457006, 'a': 12904584, 'g': 11724487}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_each_char(nonDonor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        -3 -2 -1  0  1  2  3  4  5\n",
      "0        c  t  t  t  a  a  t  t  t\n",
      "1        t  t  t  a  a  t  t  t  t\n",
      "2        t  t  a  a  t  t  t  t  a\n",
      "3        t  a  a  t  t  t  t  a  t\n",
      "4        a  a  t  t  t  t  a  t  c\n",
      "...     .. .. .. .. .. .. .. .. ..\n",
      "5511797  c  t  t  t  a  g  a  t  g\n",
      "5511798  t  t  t  a  g  a  t  g  g\n",
      "5511799  t  t  a  g  a  t  g  g  a\n",
      "5511800  t  a  g  a  t  g  g  a  g\n",
      "5511801  a  g  a  t  g  g  a  g  a\n",
      "\n",
      "[5511802 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Prior Probability:: 100%|██████████| 5511802/5511802 [02:19<00:00, 39552.68it/s]\n",
      "Calculating Conditional Probability::   0%|          | 2393/5511802 [00:00<03:50, 23929.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        p(A)      p(C)      p(G)      p(T)\n",
      "-3  0.260176  0.232064  0.236535  0.271225\n",
      "-2  0.260070  0.232180  0.236552  0.271198\n",
      "-1  0.260208  0.232249  0.236201  0.271342\n",
      " 0  0.260328  0.232092  0.236200  0.271381\n",
      " 1  0.259927  0.232399  0.236639  0.271034\n",
      " 2  0.260156  0.232376  0.236024  0.271444\n",
      " 3  0.259963  0.232284  0.236368  0.271385\n",
      " 4  0.260241  0.232256  0.236185  0.271318\n",
      " 5  0.260195  0.232187  0.236456  0.271162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Conditional Probability:: 100%|██████████| 5511802/5511802 [03:50<00:00, 23919.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      p(A,A)    p(A,C)    p(A,G)    p(A,T)    p(C,A)    p(C,C)    p(C,G)  \\\n",
      "-2  0.296746  0.187205  0.281931  0.234117  0.306976  0.294569  0.075583   \n",
      "-1  0.297091  0.187342  0.281303  0.234265  0.307077  0.294593  0.075395   \n",
      " 0  0.297018  0.187027  0.281816  0.234139  0.307245  0.294354  0.075511   \n",
      " 1  0.296868  0.187233  0.281858  0.234041  0.306103  0.295006  0.075614   \n",
      " 2  0.297431  0.187513  0.280660  0.234396  0.307051  0.294625  0.075501   \n",
      " 3  0.296638  0.187263  0.281964  0.234135  0.307043  0.294606  0.075506   \n",
      " 4  0.297221  0.187353  0.281156  0.234271  0.307098  0.294596  0.075449   \n",
      " 5  0.296928  0.187163  0.281862  0.234047  0.307079  0.294574  0.075490   \n",
      "\n",
      "      p(C,T)    p(G,A)    p(G,C)    p(G,G)    p(G,T)    p(T,A)    p(T,C)  \\\n",
      "-2  0.322872  0.257471  0.230251  0.293022  0.219256  0.187022  0.223623   \n",
      "-1  0.322935  0.257576  0.230293  0.292836  0.219296  0.187008  0.223645   \n",
      " 0  0.322891  0.257991  0.230343  0.292053  0.219613  0.187020  0.223537   \n",
      " 1  0.323277  0.258019  0.230621  0.293517  0.217843  0.186661  0.223730   \n",
      " 2  0.322823  0.257559  0.230187  0.292982  0.219271  0.186465  0.223937   \n",
      " 3  0.322844  0.257141  0.230535  0.292686  0.219638  0.186962  0.223601   \n",
      " 4  0.322857  0.257617  0.230291  0.292872  0.219220  0.186997  0.223622   \n",
      " 5  0.322857  0.257730  0.230305  0.293172  0.218793  0.186974  0.223606   \n",
      "\n",
      "      p(T,G)    p(T,T)  \n",
      "-2  0.281501  0.307854  \n",
      "-1  0.281220  0.308127  \n",
      " 0  0.281375  0.308069  \n",
      " 1  0.281472  0.308138  \n",
      " 2  0.281126  0.308472  \n",
      " 3  0.281411  0.308026  \n",
      " 4  0.281312  0.308069  \n",
      " 5  0.281325  0.308095  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "signal_to_csv(nonDonor_list,mode=\"nonDonor\") # 5510888\n",
    "priors_n=cal_priorProbability(nonDonor_list)\n",
    "prior_to_csv(priors_n,mode=\"-\")\n",
    "conditional_n = cal_conditionalProbability(nonDonor_list,priors_n)\n",
    "conditional_to_csv(conditional_n,mode=\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理部分结束！😀😀😀😀😀😀😀😀😀😀😀😀😀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
