{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d071761-53d8-484f-b705-684081f373c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install watermark\n",
    "# !pip install seaborn\n",
    "# !pip install biopython\n",
    "# !pip install sklearn\n",
    "import os\n",
    "import re \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import watermark\n",
    "import random \n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec25920-760c-44e5-885e-08bab6dfa62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "left=3\n",
    "right=9\n",
    "signal_num = left+right\n",
    "bases=\"ACGT\"\n",
    "lower_bases=\"acgtz\"\n",
    "pattern = re.compile(\"[^acgt]\")\n",
    "# output_name = f\"output/WAM(-{left}+{right})\"\n",
    "output_name = \"output/pomegranate+WAM\"\n",
    "# output_name = \"output/DWAM\"\n",
    "# output_name = \"output/pomegranate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9bf90-8ece-4617-a4b6-773c9a3d404d",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- train set contains 462 files,2831 introns, all suffies of file is \"TXT\",the letters are  lowercase  letters. The first line is \"LOCUS AB000381 35863 bp DNA PRI 14-MAY-1997\", gapped by lots of whitespace\n",
    "- test set contains 570 files,2071 introns,the suffixes of file contains \"TXT\"and \"txt\",and the letters are capital letters.The fist line in each file starts with like \">>ACU08131\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4576763b-915a-47f4-b152-5ef870035b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(dirs):\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    else:\n",
    "        pass\n",
    "mkdir(f\"{output_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd07b061-35eb-4973-9693-4a09498648fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOADING Training and testing datasets/Training Set: 100%|████████████████████████| 462/462 [00:00<00:00, 231762.76it/s]\n",
      "LOADING Training and testing datasets/Testing Set: 100%|█████████████████████████| 570/570 [00:00<00:00, 552392.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462 Files Loading Finished!\n",
      "\n",
      "570 Files Loading Finished!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def loadFile(file_dir):\n",
    "    '''\n",
    "    Function: Read  All files in the Training Set Folder and Testing Set Folder\n",
    "    Parameter：file_dir\n",
    "    Output: file_path,locus_list\n",
    "    Attention: do not load non-fasta files!\n",
    "    '''\n",
    "    file_path = []\n",
    "    file_locus_list = []\n",
    "    count=0\n",
    "    all_file = tqdm(os.listdir(file_dir), desc=f'LOADING {file_dir}')\n",
    "    for file_name in all_file:\n",
    "        count+=1\n",
    "        suffix = re.findall(\"\\.(.+$)\",file_name)[-1].lower()\n",
    "        # or  suffix = file_name.split(\".\")[1].lower()\n",
    "        if suffix != \"txt\":\n",
    "            continue\n",
    "        path = f\"{file_dir}/{file_name}\"\n",
    "        file_path.append(path)\n",
    "    all_file.write(f\"{count} Files Loading Finished!\")\n",
    "    all_file.close()\n",
    "    print()\n",
    "    return file_path\n",
    "train_file_path =loadFile('Training and testing datasets/Training Set')\n",
    "test_file_path =loadFile('Training and testing datasets/Testing Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee321300-d696-406b-b759-20a6bdd30b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Progressing：:  34%|███████████████████▌                                     | 159/462 [00:00<00:00, 1577.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Extract train Set donor signals**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train Progressing：: 100%|█████████████████████████████████████████████████████████| 462/462 [00:00<00:00, 1781.55it/s]\n",
      "test Progressing：: 100%|██████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 3572.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Dataset Train info Finished!\n",
      "**********Extract test Set donor signals**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Dataset Test info Finished!\n"
     ]
    }
   ],
   "source": [
    "def extract_donor_signal(file_path,dataset):\n",
    "    '''\n",
    "    Parameter：train_file_path|test_file_path [set_dataset]\n",
    "    Output:file_donor_positions,file_acceptor_positions,file_donor_signals,donor_signal_all\n",
    "    只输出没有含有未知碱基的site\n",
    "    '''\n",
    "    print(f'Extract {dataset} Set donor signals'.center(50, '*'))\n",
    "    donor_positions= [] #1\n",
    "    acceptor_positions= [] #1\n",
    "    donor_signals=[]  #1\n",
    "    acceptor_signals=[]   #1\n",
    "    all_donor_signal=[]  \n",
    "    all_acceptor_signal=[]\n",
    "    length_list = [] #1\n",
    "    seq_list = [] #1\n",
    "    exons = [] #1\n",
    "    locus =[]\n",
    "    donor_file = []\n",
    "    acceptor_file = []\n",
    "    files = tqdm(file_path, desc=f'{dataset} Progressing：')\n",
    "    for file in files:\n",
    "        f = open(file)\n",
    "        #  first line: extract gene locus\n",
    "        first_line =f.readline() \n",
    "        if dataset == \"test\":\n",
    "            locus.append(re.search(\">(.+)$\",first_line).group(1))\n",
    "        elif dataset==\"train\":\n",
    "            locus.append(first_line.split()[1]) \n",
    "        #  second line: extract  donor and acceptor site positions\n",
    "        second_line=f.readline()  \n",
    "        exon_positions_list = re.findall(r'(\\d+)\\.\\.(\\d+)',second_line)\n",
    "        donor_positions_list = [int(pos_set[1])+1 for pos_set in exon_positions_list[:-1]]\n",
    "        acceptor_positions_list= [int(pos_set[0])-1 for pos_set in exon_positions_list[1:]]\n",
    "        exons.append(exon_positions_list)\n",
    "\n",
    "        seq = ''\n",
    "        # extract  seq info\n",
    "        for line in f.readlines():\n",
    "            seq += line.strip()\n",
    "        seq_length = len(seq)\n",
    "        seq_list.append(seq.lower())\n",
    "        length_list.append(seq_length)\n",
    "        \n",
    "        # extract  donor site signal\n",
    "        donor_signal=[]\n",
    "        poses = []\n",
    "        for pos in donor_positions_list:\n",
    "            signal_range = seq[pos-1-left:pos-1+right].lower()\n",
    "            no_known =pattern.search(signal_range)\n",
    "            if no_known:\n",
    "                    continue\n",
    "            donor_signal.append(signal_range)\n",
    "            all_donor_signal.append(signal_range)\n",
    "            donor_file.append(file)\n",
    "            poses.append(pos)\n",
    "        donor_signals.append(donor_signal)\n",
    "        donor_positions.append(poses)\n",
    "        \n",
    "        # extract  acceptor site signal\n",
    "        acceptor_signal=[]\n",
    "        poses = []\n",
    "        for pos in acceptor_positions_list:\n",
    "            signal_range = seq[pos-right:pos+left].lower()\n",
    "            no_known =pattern.search(signal_range)\n",
    "            if no_known:\n",
    "                    continue\n",
    "            poses.append(pos)\n",
    "            acceptor_signal.append(signal_range)\n",
    "            all_acceptor_signal.append(signal_range)\n",
    "            acceptor_file.append(file)\n",
    "        acceptor_signals.append(acceptor_signal)\n",
    "        acceptor_positions.append(acceptor_positions_list)\n",
    "        \n",
    "    # save info to file\n",
    "    df_set_info = pd.DataFrame({'Path':file_path, 'Locus':locus,\"Length\":length_list,\"Exon Num\":[ len(exons) for exons  in exons],\\\n",
    " \"Exon Location\":exons,\"Donor Site\":donor_positions,\"Acceptor Site\":acceptor_positions,\\\n",
    "                                \"Donor signals\":donor_signals,\"Acceptor signals\":acceptor_signals})\n",
    "    df_set_info.to_csv(f'{output_name}/{dataset.capitalize()}_set_info(non-seq).csv',index=None)\n",
    "    np.savetxt(f'{output_name}/{dataset.capitalize()}_seq_list.txt',seq_list,delimiter = ',',fmt='%s')\n",
    "    np.savetxt(f'{output_name}/{dataset.capitalize()}_donor_signal_str.txt',all_donor_signal,delimiter = ',',fmt='%s')\n",
    "    print(f\"Extract Dataset {dataset.capitalize()} info Finished!\")\n",
    "    if dataset==\"train\":\n",
    "        return all_donor_signal,all_acceptor_signal,seq_list,donor_positions,acceptor_positions\n",
    "    elif dataset==\"test\":\n",
    "        return all_donor_signal,all_acceptor_signal,seq_list,donor_positions,acceptor_positions,donor_file\n",
    "\n",
    "train_donor_signal_all_str,train_acceptor_signal_all_str,train_seq_list,\\\n",
    "train_donor_positions,train_acceptor_positions=extract_donor_signal(train_file_path,dataset=\"train\")\n",
    "test_donor_signal_all_str,test_acceptor_signal_all_str,test_seq_list,\\\n",
    "test_donor_positions,test_acceptor_positions,test_donor_filepath=extract_donor_signal(test_file_path,dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb86933-a42d-4a1a-a790-8b810e4d5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_to_csv(signal_str,mode,folder=\"Train\"):\n",
    "    \"\"\"\n",
    "    output:  csv\n",
    "    \"\"\"\n",
    "    signal_list= map(list, signal_str)\n",
    "    if mode==\"acceptor\":\n",
    "        col_name = list(range(-right+1,left+1))\n",
    "    else:\n",
    "        col_name = list(range(-left,right))\n",
    "   \n",
    "    donorDf = pd.DataFrame(columns=col_name, data=signal_list, index=None)\n",
    "    donorDf.to_csv(f'{output_name}/{folder}_{mode}_signal.csv',index=None)\n",
    "    return donorDf\n",
    "def save_str_list(signal_str,filename=\"signal_str\"):\n",
    "    np.savetxt(f'{output_name}/{filename}.txt',signal_str,delimiter = ',',fmt='%s')\n",
    "    print(f'save {filename} successful!')\n",
    "\n",
    "train_signal = signal_to_csv(train_donor_signal_all_str,folder=\"Train\",mode=\"donor\")\n",
    "test_signal = signal_to_csv(test_donor_signal_all_str,folder=\"Test\",mode=\"donor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d00c5a9-2d18-4d1b-826d-87ef679cec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting  Base Distribution:: 100%|█████████████████████████████████████████████████| 462/462 [00:01<00:00, 451.50it/s]\n",
      "Counting  Base Distribution:: 100%|████████████████████████████████████████████| 2380/2380 [00:00<00:00, 340837.32it/s]\n",
      "Counting  Base Distribution:: 100%|████████████████████████████████████████████████| 570/570 [00:00<00:00, 1099.00it/s]\n",
      "Counting  Base Distribution:: 100%|████████████████████████████████████████████| 2079/2079 [00:00<00:00, 417103.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of all bases in training set\n",
      " {'g': 1306260, 'c': 1282733, 'a': 1436966, 't': 1498203, 'n': 682, 'k': 28, 's': 27, 'r': 15, 'y': 26, 'w': 14, 'm': 16, 'v': 2, 'b': 4}\n",
      "Distribution of donor site bases in training  set\n",
      " {'a': 7301, 'g': 11043, 't': 6245, 'c': 3971}\n",
      "Distribution of all bases in testing set\n",
      " {'c': 689433, 't': 762900, 'g': 702343, 'a': 736600, 'n': 862, 'b': 1, 'r': 5, 'y': 3, 'k': 1, 's': 1}\n",
      "Distribution of donor site  bases in testing set\n",
      " {'g': 9414, 't': 5642, 'a': 6506, 'c': 3386}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def count_each_char(str_list):\n",
    "    dict = {}\n",
    "    str_list = tqdm(str_list, desc='Counting  Base Distribution:')\n",
    "    for seq in str_list:\n",
    "         for i  in seq:\n",
    "            if i not in dict:\n",
    "                dict[i] = 1\n",
    "            else:\n",
    "                dict[i] += 1\n",
    "    return dict\n",
    "base_dis_1 = count_each_char(train_seq_list)\n",
    "base_dis_2 = count_each_char(train_donor_signal_all_str)\n",
    "base_dis_3 = count_each_char(test_seq_list)\n",
    "base_dis_4 = count_each_char(test_donor_signal_all_str)\n",
    "print(\"Distribution of all bases in training set\\n\",base_dis_1)\n",
    "print(\"Distribution of donor site bases in training  set\\n\",base_dis_2)\n",
    "print(\"Distribution of all bases in testing set\\n\",base_dis_3)\n",
    "print(\"Distribution of donor site  bases in testing set\\n\",base_dis_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bdf5820-2175-4257-b71d-fc3b405febd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code_all_signal:: 100%|████████████████████████████████████████████████████████| 2380/2380 [00:00<00:00, 216971.91it/s]\n"
     ]
    }
   ],
   "source": [
    "def sub_unknown(my_string):\n",
    "    \"\"\"\n",
    "    function to convert a DNA sequence string to a numpy array\n",
    "    converts to lower case, changes any non 'acgt' characters to 'n'\n",
    "    like: ['c' 'a' 't' 'g' 'g']\n",
    "    \"\"\"\n",
    "    my_string = pattern.sub('z', my_string)\n",
    "    return my_string \n",
    "\n",
    "\n",
    "def process_to_int(donor):\n",
    "    donor = list(sub_unknown(donor))\n",
    "    integer_encoded = [char_to_int[char] for char in donor]\n",
    "    return list(integer_encoded)\n",
    "\n",
    "def code_all_seq(all_str):\n",
    "    all_str = tqdm(all_str, desc='code_all_signal:')\n",
    "    int_coded=map(process_to_int,all_str)\n",
    "    return np.array(list(int_coded))\n",
    "\n",
    "char_to_int = {c:i for i, c in enumerate(lower_bases)}\n",
    "train_coded= code_all_seq(train_donor_signal_all_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37001abf-1b0a-4abe-9e40-2548f9b2eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Pseudo Donor Signal Sequence:: 100%|███████████████████████████████████████| 462/462 [00:02<00:00, 205.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Pseudo Donor Signal Sequence successful!\n",
      "283482\n",
      "save Train_pseudoDonor_signal_str successful!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_pseudoDonor(file_path,seqs_DNA, donor_locations,dataset,ran_num=0):\n",
    "    '''\n",
    "    output :pseudo donor signal containing 'gt' in the right position\n",
    "    '''\n",
    "    nonDonors = []\n",
    "    nonDonor_file_path = []\n",
    "    nonDonor_positions = []\n",
    "    file_num = tqdm(range(len(donor_locations)), desc='Creating Pseudo Donor Signal Sequence:')\n",
    "    for i in file_num:\n",
    "        file_nonDonors= []\n",
    "        file_seq_DNA = seqs_DNA[i]\n",
    "        num = len(donor_locations[i])  \n",
    "        length = len(file_seq_DNA)\n",
    "        donor_signals_start=[pos-1-left for pos in donor_locations[i]]\n",
    "        for index in range(length-signal_num+1):\n",
    "            if (file_seq_DNA[index+left:index+left+2] =='gt' ) and (index not in donor_signals_start) :\n",
    "                nonDonor = file_seq_DNA[index:index + signal_num]\n",
    "                no_known =pattern.search(nonDonor)\n",
    "                if no_known:\n",
    "                    continue # 这里之前写成break，有问题，这样遇到非正常碱基对的就直接循环中停止了\n",
    "                file_nonDonors.append(nonDonor)\n",
    "                nonDonor_file_path.append(file_path[i])\n",
    "                nonDonor_positions.append(index+1+left)\n",
    "        if ran_num:\n",
    "            nonDonors += random.sample(file_nonDonors,ran_num)\n",
    "        else:\n",
    "            nonDonors += file_nonDonors\n",
    "        # nonDonors.append(random.sample(file_nonDonors,ran_num))\n",
    "    print('Created Pseudo Donor Signal Sequence successful!')\n",
    "    if dataset == \"train\":\n",
    "         return nonDonors\n",
    "    elif dataset == \"test\":\n",
    "        return nonDonors,nonDonor_file_path,nonDonor_positions\n",
    "\n",
    "# 生成训练集假位点\n",
    "train_pseudoDonor_list=create_pseudoDonor(train_file_path,train_seq_list, train_donor_positions,dataset = \"train\")\n",
    "train_pseudoDonor_len= len(train_pseudoDonor_list) \n",
    "print(train_pseudoDonor_len) \n",
    "save_str_list(train_pseudoDonor_list,\"Train_pseudoDonor_signal_str\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3773d86-6d03-4cce-b46b-a63a1e871f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Pseudo Donor Signal Sequence:: 100%|███████████████████████████████████████| 570/570 [00:01<00:00, 472.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Pseudo Donor Signal Sequence successful!\n",
      "149126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 生成测试集假位点\n",
    "test_pseudoDonor_list,test_pseudoDonor_filepath,test_pseudoDonor_positions=create_pseudoDonor(test_file_path,test_seq_list,\\\n",
    "                                                                                      test_donor_positions,dataset = \"test\")\n",
    "test_pseudoDonor_len= len(test_pseudoDonor_list)\n",
    "print(test_pseudoDonor_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ca4ca66-8939-4667-834b-e4f2cb0ce561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机抽取训练集样本\n",
    "import random \n",
    "random.seed(123123)\n",
    "random_len = 0\n",
    "if random_len:\n",
    "    train_pseudoDonor_list_part = random.sample(train_pseudoDonor_list, random_len)\n",
    "else:\n",
    "    train_pseudoDonor_list_part =train_pseudoDonor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d097774-4c34-4a11-b3da-a4bfb8d4db7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code_all_signal:: 100%|████████████████████████████████████████████████████████| 2380/2380 [00:00<00:00, 148858.39it/s]\n",
      "code_all_signal:: 100%|████████████████████████████████████████████████████| 283482/283482 [00:00<00:00, 329598.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# 得到训练集的信息df\n",
    "train_donor_features = code_all_seq(train_donor_signal_all_str)\n",
    "train_labels=[1]*len(train_donor_signal_all_str)\n",
    "train_pseudoDonor_features = code_all_seq(train_pseudoDonor_list_part)\n",
    "train_labels += [0]*len(train_pseudoDonor_list_part)\n",
    "train_labels = np.array(train_labels) \n",
    "train_features =np.vstack([train_donor_features,train_pseudoDonor_features])\n",
    "\n",
    "po_str = list(map(str,list(range(-left,right))))\n",
    "train_features_df = pd.DataFrame(columns=po_str, data=train_features , index=None)\n",
    "train_features_df[\"Label\"] = train_labels\n",
    "# train_donor_df = train_features_df[train_df[\"Label\"] == 1]\n",
    "# train_pseudo_df = train_features_df[train_df[\"Label\"] == 0]\n",
    "train_features_df.to_csv(f\"{output_name}/Train_features.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3a4d752-669c-46b7-a44f-0f525a57838b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Donor Site</th>\n",
       "      <th>Signal</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Training and testing datasets/Testing Set/ACU0...</td>\n",
       "      <td>642</td>\n",
       "      <td>ggggtgagccca</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Training and testing datasets/Testing Set/ACU0...</td>\n",
       "      <td>1363</td>\n",
       "      <td>gtggtaagagac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Training and testing datasets/Testing Set/ACU0...</td>\n",
       "      <td>2029</td>\n",
       "      <td>taggtgagtgtg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Training and testing datasets/Testing Set/ACU0...</td>\n",
       "      <td>2803</td>\n",
       "      <td>gcggtaggtact</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Training and testing datasets/Testing Set/ACU0...</td>\n",
       "      <td>3798</td>\n",
       "      <td>caggtaattttc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151200</th>\n",
       "      <td>Training and testing datasets/Testing Set/ZEFB...</td>\n",
       "      <td>2123</td>\n",
       "      <td>taagttaaatca</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151201</th>\n",
       "      <td>Training and testing datasets/Testing Set/ZEFB...</td>\n",
       "      <td>2145</td>\n",
       "      <td>atagtggcctac</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151202</th>\n",
       "      <td>Training and testing datasets/Testing Set/ZEFB...</td>\n",
       "      <td>2159</td>\n",
       "      <td>tgagtttctgtt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151203</th>\n",
       "      <td>Training and testing datasets/Testing Set/ZEFB...</td>\n",
       "      <td>2165</td>\n",
       "      <td>tctgttatgtgg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151204</th>\n",
       "      <td>Training and testing datasets/Testing Set/ZEFB...</td>\n",
       "      <td>2170</td>\n",
       "      <td>tatgtggctaac</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151205 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Filename  Donor Site  \\\n",
       "0       Training and testing datasets/Testing Set/ACU0...         642   \n",
       "1       Training and testing datasets/Testing Set/ACU0...        1363   \n",
       "2       Training and testing datasets/Testing Set/ACU0...        2029   \n",
       "3       Training and testing datasets/Testing Set/ACU0...        2803   \n",
       "4       Training and testing datasets/Testing Set/ACU0...        3798   \n",
       "...                                                   ...         ...   \n",
       "151200  Training and testing datasets/Testing Set/ZEFB...        2123   \n",
       "151201  Training and testing datasets/Testing Set/ZEFB...        2145   \n",
       "151202  Training and testing datasets/Testing Set/ZEFB...        2159   \n",
       "151203  Training and testing datasets/Testing Set/ZEFB...        2165   \n",
       "151204  Training and testing datasets/Testing Set/ZEFB...        2170   \n",
       "\n",
       "              Signal  label  \n",
       "0       ggggtgagccca      1  \n",
       "1       gtggtaagagac      1  \n",
       "2       taggtgagtgtg      1  \n",
       "3       gcggtaggtact      1  \n",
       "4       caggtaattttc      1  \n",
       "...              ...    ...  \n",
       "151200  taagttaaatca      0  \n",
       "151201  atagtggcctac      0  \n",
       "151202  tgagtttctgtt      0  \n",
       "151203  tctgttatgtgg      0  \n",
       "151204  tatgtggctaac      0  \n",
       "\n",
       "[151205 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得测试集的信息df\n",
    "import operator\n",
    "from functools import reduce\n",
    "# 把多维列表变为一维\n",
    "test_donor_positions_1d = reduce(operator.add,test_donor_positions)\n",
    "test_labels=[1]*len(test_donor_signal_all_str)\n",
    "test_labels += [0]*test_pseudoDonor_len\n",
    "test_labels = np.array(test_labels)\n",
    "test_file_df = pd.DataFrame({\"Filename\":test_donor_filepath+test_pseudoDonor_filepath,\"Donor Site\":test_donor_positions_1d +test_pseudoDonor_positions,\\\n",
    "              \"Signal\":test_donor_signal_all_str+test_pseudoDonor_list,\"label\":test_labels})\n",
    "test_file_df.to_csv(f\"{output_name}/Test_predict.csv\",index=None)\n",
    "test_file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ce8e2bc-59c3-40ba-8e4d-a8d8f7190f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "code_all_signal:: 100%|████████████████████████████████████████████████████| 151205/151205 [00:00<00:00, 251471.87it/s]\n"
     ]
    }
   ],
   "source": [
    "test_features = code_all_seq(test_file_df['Signal'])\n",
    "test_features_df= pd.DataFrame(columns=po_str, data=test_features , index=None)\n",
    "test_features_df.to_csv(f\"{output_name}/Test_features.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8559e38c-8f1f-4433-8be4-d81b438c032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_commonSignal(seqs_DNA, donor_locations, acceptor_locations):\n",
    "\n",
    "#     nonDonors = []\n",
    "#     file_num = tqdm(range(len(donor_locations)), desc='Creating Non Donor Signal Sequence:')\n",
    "#     for i in file_num:\n",
    "#         # 每个文件循环\n",
    "#         file_seq_DNA = seqs_DNA[i]\n",
    "#         num = len(donor_locations[i])  \n",
    "#         length = len(file_seq_DNA)\n",
    "#         donor_signals_start=[pos-1-left for pos in donor_locations[i]]\n",
    "#         acceptor_signals_start=[pos-right for pos in acceptor_locations[i] ]\n",
    "#         signals_start=sorted(donor_signals_start+acceptor_signals_start)\n",
    "#         for index in range(length-signal_num+1):\n",
    "#             if index not in signals_start:\n",
    "#                 nonDonor = file_seq_DNA[index:index + signal_num]\n",
    "#                 no_known =pattern.search(nonDonor)\n",
    "#                 if no_known:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     yield nonDonor\n",
    "\n",
    "#     print('Created Common Signal Sequence successful!')\n",
    "# common_list=create_commonSignal(train_seq_list, train_donor_positions, train_acceptor_positions)\n",
    "# common_list=list(common_list)\n",
    "# common_array=code_all_seq(common_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "295a3699-21e4-405d-b84b-712d2bfa7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import winsound\n",
    "# duration = 1000  # millisecond\n",
    "# freq = 440  # Hz\n",
    "# winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e70f2e-0edd-45f4-b69e-3d409c37c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notify_run import Notify\n",
    "\n",
    "# n = Notify()\n",
    "# n.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bac23a5a-dc2e-4c93-abaf-2b6ec89e565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee7585df-d0c6-4ce2-b50e-eb5eeb8369b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notify_run import Notify\n",
    "n = Notify()\n",
    "n.send(f\"Finished {output_name} preprocesing~!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e52feb-ae76-4565-b06f-2526497cb7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1dce94-058b-4919-b824-a13540289932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
